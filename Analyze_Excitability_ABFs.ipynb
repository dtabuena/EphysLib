{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crispy_Analyze_ABF_files.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/EphysLib/blob/main/Analyze_Excitability_ABFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### SORCE DATA: RNF182 Dropbox\n",
        "link = \"https://www.dropbox.com/sh/n9t8p257wnzlijk/AAC9Z36JodisyZjnM3mkJC3Xa?dl=0\""
      ],
      "metadata": {
        "id": "s-aIbCWF-RlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKwnij3xH0D7"
      },
      "outputs": [],
      "source": [
        "################ Helpers ######################\n",
        "def print_assert():\n",
        "    'Print assertion failure messages (typicaly when try/except is employed)'\n",
        "    import sys\n",
        "    import traceback\n",
        "    _, _, tb = sys.exc_info()\n",
        "    # traceback.print_tb(tb) # Fixed format\n",
        "    tb_info = traceback.extract_tb(tb)\n",
        "    filename, line, func, text = tb_info[-1]\n",
        "    print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def flatten(list_of_lists):\n",
        "    'Recursively Flatten list of lists'\n",
        "    if len(list_of_lists) == 0:\n",
        "        return list_of_lists\n",
        "    if isinstance(list_of_lists[0], list):\n",
        "        return flatten(list_of_lists[0]) + flatten(list_of_lists[1:])\n",
        "    return list_of_lists[:1] + flatten(list_of_lists[1:])\n",
        "\n",
        "\n",
        "def protocol_baseline_and_stim(abf):\n",
        "    'Return two boolean arrays, distiguishing holding I/V and electrical stimuli'\n",
        "    # use command signal variance to determine stimulus periods\n",
        "    commands = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=s)\n",
        "        commands.append(abf.sweepC)\n",
        "    commands = np.stack(commands)\n",
        "    \n",
        "    std = np.std(commands, axis=0)\n",
        "    is_base = std==0\n",
        "    is_stim = np.logical_not(is_base)\n",
        "    return is_base, is_stim\n",
        "\n",
        "def abf_or_name(abf):\n",
        "    if str(type(abf))==\"<class 'str'>\":\n",
        "        abf = pyabf.ABF( abf )\n",
        "    return abf\n",
        "\n",
        "def plot_sweeps_and_command(abf,figsize = [8,2],windows=[]):\n",
        "    'Plot an abf file with sweeps and command'\n",
        "    'also attempts to calibrate telegraph offset when a Ch1 is a secondary ouput of the amp'\n",
        "    abf = abf_or_name(abf)\n",
        "    axs_2_right = []\n",
        "    # plot sweeps and command (single channel)\n",
        "\n",
        "\n",
        "    discretized_cmap = matplotlib.cm.get_cmap('viridis', len(abf.sweepList))\n",
        "\n",
        "    num_ch = 1;\n",
        "    if len(abf.channelList)>1:\n",
        "        num_ch = 2\n",
        "    fig, axs = plt.subplots(num_ch, figsize = np.array(figsize)*np.array([1,num_ch]))\n",
        "    if num_ch ==1:\n",
        "        axs=[axs]\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    if num_ch==2:\n",
        "        axs1_r = axs[1].twinx()\n",
        "        axs[1].set_ylabel(str('Ch1 '+str(abf.sweepLabelC)))\n",
        "        axs1_r.set_ylabel(str('Corrrected '+str(abf.sweepLabelC)[-4:]) )\n",
        "        axs[1].set_title(str(theta) +','+ str(offset))\n",
        "        axs[1].set_title('Command Correction')\n",
        "    axs0_r = axs[0].twinx()\n",
        "    for sweep in abf.sweepList:\n",
        "        abf.setSweep(sweepNumber=sweep)\n",
        "        axs[0].plot(abf.sweepX, abf.sweepY,color=discretized_cmap(sweep))        \n",
        "        axs0_r.plot(abf.sweepX, abf.sweepC,color='grey' )#discretized_cmap(sweep),linestyle='dotted')\n",
        "        com = abf.sweepC\n",
        "        if len(abf.channelList)>1:\n",
        "            abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "            axs[1].plot(abf.sweepX, abf.sweepY,'m')\n",
        "            corrected_com = correct_ch1(theta,abf.sweepY)\n",
        "            axs1_r.plot(abf.sweepX, corrected_com,'c',linestyle='dotted')\n",
        "            # axs_2.set_title( str(theta)+','+ str(correct_ch1(theta,abf.sweepY[0])) )\n",
        "    axs[0].set_title(abf.abfID)\n",
        "    abf.setSweep(0)\n",
        "    axs[0].set_ylabel(str(abf.sweepLabelY))\n",
        "    axs[0].set_xlabel(str(abf.sweepLabelX))\n",
        "    axs0_r.set_ylabel(str(abf.sweepLabelC))\n",
        "    axs0_r.set_xlabel(str(abf.sweepLabelX))\n",
        "\n",
        "    cmap = matplotlib.cm.get_cmap('Dark2')\n",
        "    color_num=0\n",
        "    for w in windows:\n",
        "        color_num+=1\n",
        "        rgba = cmap(color_num/len(windows))\n",
        "        ylim = axs[0].get_ylim()\n",
        "        sr = np.mean(np.diff( w ))\n",
        "        gaps = np.where(np.diff(w)>sr*4)[0]\n",
        "        sw = flatten([w[0],[ list(w[[i,i+1]]) for i in np.arange(len(w)-1) if np.diff(w)[i] > sr*3 ] , w[-1] ])\n",
        "        for i in range(0, len(sw), 2):\n",
        "            axs[0].axvspan(sw[i], sw[i+1], ylim[0], ylim[1], alpha=0.2,color=rgba)\n",
        "\n",
        "\n",
        "    axs[0].set_zorder(1)  # default zorder is 0 for ax1 and ax2\n",
        "    axs[0].patch.set_visible(False)  # prevents ax1 from hiding ax2\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "    return fig, axs , theta\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lV9qOxbL-P2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################# Math ######################\n",
        "import math\n",
        "\n",
        "\n",
        "def movmean(x, w):\n",
        "    'A moving mean filter'\n",
        "    w = int(w)\n",
        "    # plt.plot(np.arange(len(x)) - int(len(x)/2), x)\n",
        "    if len(x) < w: w = len(x)\n",
        "    px = np.pad(x,int(np.ceil((w-1)/2)),'edge') \n",
        "    if len(px)-len(x) == w: px = px[0:-1]\n",
        "    conv = np.convolve(px, np.ones(w), 'valid') / w\n",
        "    return conv\n",
        "\n",
        "def mono_exp(time, peak, tau, ss):\n",
        "    'A single exponential decay function'\n",
        "    return (peak * np.exp(-time / (tau)) + ss)\n",
        "\n",
        "def rms_noise(x):\n",
        "    return np.sqrt(    np.sum((x-x.mean())**2)/len(x)    )   \n",
        "\n",
        "def predict_telegraph_DEFUNCT(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "            theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            # print(theta,'theta_1')\n",
        "            modes = []\n",
        "            steps = list(set(all_ch0_com))\n",
        "            steps.sort()\n",
        "            steps = np.array(steps)\n",
        "            for s in steps:\n",
        "                mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )[0]\n",
        "                modes.append(mode)\n",
        "            modes = np.concatenate(modes)\n",
        "            m_ind = [i for i in np.arange(len(modes)) if modes[i]<-40]\n",
        "            steps = steps[m_ind]\n",
        "            modes = modes[m_ind]\n",
        "            theta = np.polyfit(steps, modes, 1)\n",
        "            # print(theta,'theta_2')\n",
        "            if to_plot:\n",
        "                plt.scatter(steps,modes)\n",
        "                y_hat = steps*theta[0]+theta[1]\n",
        "                plt.plot(steps,  y_hat )\n",
        "                plt.show()\n",
        "            if to_plot:\n",
        "                for s in steps:\n",
        "                    counts, bin_edges = np.histogram( all_ch1_y[all_ch0_com==s],bins=50,range=(-120,70),density=True )\n",
        "                    plt.plot(bin_edges[1:],counts)\n",
        "                    mode = scipy.stats.mode( all_ch1_y[all_ch0_com==s] )\n",
        "                    # plt.scatter(s,mode)\n",
        "                plt.gca().xaxis.set_major_locator(plt.MultipleLocator(10))\n",
        "                plt.gca().grid()\n",
        "                plt.show()\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    if stabilize_one_to_one:\n",
        "        # if np.round(theta[0],1)==1.0:\n",
        "        theta = [theta[0] , round(theta[1]/v_res)*v_res ]\n",
        "        if theta[0]>1: theta[0]=math.floor(theta[0])\n",
        "\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(t,signal):\n",
        "        return t[1]/t[0] + (signal - t[1]) / t[0]\n",
        "    return theta, offset, correct_ch1\n",
        "\n",
        "def predict_telegraph(abf,to_plot=False,stabilize_one_to_one = True,v_res = 10):\n",
        "    'Uses the difference between the voltage protocol command and'\n",
        "    'Secondary ouput channel to calcualte the offset and gain between'\n",
        "    'amplifier and digitizer'\n",
        "    abf = abf_or_name(abf)\n",
        "    theta = [1,0]\n",
        "    try:\n",
        "        if len(abf.channelList)>1:\n",
        "            all_ch1_y =[]\n",
        "            all_ch0_com =[]\n",
        "            for sweep in abf.sweepList:\n",
        "                abf.setSweep(sweepNumber=sweep, channel=1)\n",
        "                all_ch1_y.append(abf.sweepY)\n",
        "                abf.setSweep(sweepNumber=sweep, channel=0)\n",
        "                all_ch0_com.append(abf.sweepC)\n",
        "            all_ch1_y = np.concatenate(all_ch1_y)\n",
        "            all_ch0_com = np.concatenate(all_ch0_com)\n",
        "\n",
        "            test_theta = np.polyfit(all_ch0_com, all_ch1_y, 1)\n",
        "            y_hat = test_theta[1]/test_theta[0] + (all_ch0_com - test_theta[1]) / test_theta[0]\n",
        "            err = abs(y_hat-all_ch1_y)\n",
        "            small_err = err<(np.mean(err)+0.2*np.std(err))\n",
        "            if to_plot:\n",
        "                plt.scatter(all_ch0_com[small_err],all_ch1_y[small_err])\n",
        "                plt.show()\n",
        "            theta = np.polyfit(all_ch0_com[small_err], all_ch1_y[small_err], 1)\n",
        "    except:\n",
        "        'keep default theta'\n",
        "    offset = theta[1]/theta[0]\n",
        "    def correct_ch1(theta,signal):\n",
        "        return theta[1]/theta[0] + (signal - theta[1]) / theta[0]\n",
        "    return theta, offset, correct_ch1"
      ],
      "metadata": {
        "id": "IFLDC-cJGMWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Importing ABFs From DropBox ################\n",
        "\n",
        "def get_drobox_folder(link, new_filename):\n",
        "    'Download a folder from dropbox and unzip'\n",
        "\n",
        "    suffix_start = new_filename.find(\".zip\")\n",
        "    new_filename_stripped = new_filename[0:suffix_start]\n",
        "    zipped_file_path = \"/content/\"+new_filename\n",
        "    unzipped_file_path = \"/content/\"+new_filename_stripped\n",
        "    if not( os.path.exists(zipped_file_path)):\n",
        "        !wget -O $new_filename $link    # download with new name\n",
        "    # if not( os.path.exists(new_filename_stripped)):\n",
        "    !echo A | unzip $zipped_file_path -d $unzipped_file_path \n",
        "    return new_filename_stripped\n",
        "\n",
        "def get_sub_files(rootdir):\n",
        "    'Recursively search subfolders and return a list of all files'\n",
        "    file_list =[]\n",
        "    for rootdir, dirs, files in os.walk(file_loc): \n",
        "            file_list.extend([os.path.join(rootdir,f) for f in files])\n",
        "    return file_list\n",
        "\n",
        "\n",
        "\n",
        "def catalogue_recs(file_loc,cell_id_order):\n",
        "    'Read metadata from abf files stored in chosen folder and assigns'\n",
        "    'them to a dataframe for further processing. All further abf analyses'\n",
        "    'read files from this df and report values in the df.'\n",
        "\n",
        "    file_list = get_sub_files(file_loc)\n",
        "    # file_list = [file_loc+'/'+f for f in file_list]\n",
        "\n",
        "    file_list=[f for f in file_list if '.abf' in f]\n",
        "\n",
        "    abf_recordings_df = pd.DataFrame(data = file_list, columns=['file_name'])    \n",
        "    abf_recordings_df = abf_recordings_df.set_index('file_name')\n",
        "\n",
        "    abf_recordings_df['Recording_name'] = None\n",
        "    abf_recordings_df['cell_id'] = None\n",
        "    for c in cell_id_order:\n",
        "        abf_recordings_df[c] = None\n",
        "    \n",
        "    abf_recordings_df[\"protocol\"] = None\n",
        "    abf_recordings_df[\"abf_timestamp\"] = None\n",
        "    abf_recordings_df[\"channelList\"] = None\n",
        "\n",
        "\n",
        "    for r in np.arange(len(abf_recordings_df)):\n",
        "        row_filename = abf_recordings_df.index[r]\n",
        "        if '.sta' in row_filename:\n",
        "            continue\n",
        "        base_name = os.path.basename(row_filename)\n",
        "        abf_recordings_df.loc[row_filename,'Recording_name'] = base_name\n",
        "        split_words = base_name.split('_')\n",
        "        re_code = ['_'+split_words[i] for i in range(len(cell_id_order))]\n",
        "        re_code = ''.join(re_code)[1:]\n",
        "        abf_recordings_df.loc[row_filename,'cell_id'] = re_code\n",
        "        for ci in range(len(cell_id_order)):\n",
        "            abf_recordings_df.loc[row_filename,cell_id_order[ci]] = split_words[ci]\n",
        "\n",
        "        abf = pyabf.ABF(row_filename)\n",
        "        abf_recordings_df.loc[row_filename,'protocol'] = abf.protocol\n",
        "        abf_recordings_df.at[row_filename,'channelList'] = abf.channelList\n",
        "        abf_recordings_df.at[row_filename,'abf_timestamp'] = abf.abfDateTimeString\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    protocol_set = list(set(abf_recordings_df['protocol']))\n",
        "    return abf_recordings_df, protocol_set\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_pulses(command):\n",
        "    'Searches a command signal for start and stop times'\n",
        "    'of stimuli'\n",
        "    is_base = command==command[0]\n",
        "    is_step = np.logical_not(is_base)\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "    return starts, stops\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R67DNyK1GReQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "##########################################################\n",
        "##########  FIRING RATE GAIN ################\n",
        "\n",
        "def IF_curve_analysis(abf_recordings_df,protocol_aliases,R2_thresh = 0.8, to_plot = 0 ):\n",
        "    'Loop through all abfs finding approproate voltage protocols and calculates the firing rate gain, spikes per pA of all'\n",
        "\n",
        "    abf_recordings_df['Gain_Stims_pA'] = None\n",
        "    abf_recordings_df['Gain_NumSpikes'] = None\n",
        "    abf_recordings_df['Gain_Stims_pA'] = abf_recordings_df['Gain_Stims_pA'].astype(object)\n",
        "    abf_recordings_df['Gain_NumSpikes'] = abf_recordings_df['Gain_NumSpikes'].astype(object)\n",
        "    abf_recordings_df['v_before_stim'] = None\n",
        "    abf_recordings_df['Firing_Duration_%'] = np.nan\n",
        "    \n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "        # try:\n",
        "        gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "\n",
        "        abf_recordings_df.at[file_name,'Gain_Stims_pA'] = stim_currents\n",
        "        abf_recordings_df.at[file_name,'Gain_NumSpikes'] = spike_counts\n",
        "        abf_recordings_df.at[file_name,'v_before_stim'] = np.mean(v_before_stim)\n",
        "        abf_recordings_df.at[file_name,'Firing_Gain_(Hz/pA)'] = gain_slope\n",
        "        abf_recordings_df.at[file_name,'R2 (Firing_Gain_R2)'] = R2\n",
        "        abf_recordings_df.at[file_name,'Firing_Duration_%'] = np.nanmedian(fire_dur)\n",
        "        # except: \n",
        "        #     print('\\n' + 'unknown error on ', file_name)\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_gain_abf(file_name,spike_args,R2_thresh = 0.8, to_plot = 0):\n",
        "    # print(\"#\" , list(abf_recordings_df.index).index(file_name))\n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if len(abf.sweepList)<5: #print( 'not enough sweeps')\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan , np.nan \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    stim_currents, spike_counts, spike_rates,_,v_before_stim, fire_dur = spikes_per_stim(abf,spike_args, mode='count', to_plot=to_plot)\n",
        "    if sum(spike_counts)==0:    #if no spikes return none\n",
        "        return np.nan, np.nan, np.nan, np.nan, np.nan , np.nan \n",
        "    if_fit = fit_firing_gain( stim_currents, spike_counts, spike_rates ,to_plot=to_plot>0)\n",
        "    gain_slope = if_fit['slope']\n",
        "    R2 = if_fit['R2']\n",
        "\n",
        "    # 'all or none fail'\n",
        "    value_list = [gain_slope, R2, stim_currents, spike_counts, v_before_stim]    \n",
        "    is_nan_val = [np.isnan(v) for v in value_list]\n",
        "    is_nan_val = [v if len_one(v)==1 else any(v) for v in is_nan_val ]\n",
        "\n",
        "    if is_nan_val.count(True) > 0:\n",
        "        val_names = ['gain_slope', 'R2', 'stim_currents', 'spike_counts', 'v_before_stim']\n",
        "        bad_vals = [val_names[i] for i in range(len(is_nan_val)) if is_nan_val[i]]\n",
        "        print('\\n')\n",
        "        print('one fail all on ', file_name)\n",
        "        print('     ', bad_vals, ' = nan')\n",
        "        \n",
        "        gain_slope = np.nan\n",
        "        R2 = np.nan\n",
        "        stim_currents = np.nan\n",
        "        spike_counts = np.nan\n",
        "        v_before_stim = np.nan\n",
        "        fire_dur = np.nan\n",
        "\n",
        "\n",
        "    return gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur\n",
        "\n",
        "def len_one(x):\n",
        "    try:\n",
        "        return len(x)\n",
        "    except:\n",
        "        return 1\n",
        "        \n",
        "\n",
        "def spikes_per_stim(abf,spike_args,thresh=20,mode='count', to_plot=0):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    fire_dur = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,is_stim=is_stim,mode='count',to_plot=to_plot)\n",
        "        rel_firing_duration = check_inactivation( abf.sweepX, abf.sweepY, is_stim, abf.sampleRate, dVds, inds, mean_spike_rate, to_plot=0 )\n",
        "        # plot id'd spikes\n",
        "        if to_plot>1:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red',zorder=2)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY,zorder=1)\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "        fire_dur.append(rel_firing_duration)\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim) , np.array(fire_dur)\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=0):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    spike_thresh = spike_args['spike_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>spike_thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if to_plot>2:\n",
        "        fig1, axs1 = plt.subplots(1,figsize = [9,2])\n",
        "        axs1.plot(np.arange(len(dVds))/rate,dVds,zorder=1)\n",
        "        axs1.scatter((np.arange(len(dVds))/rate)[inds],dVds[inds],color='red',zorder=2)\n",
        "        plt.show()\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate\n",
        "\n",
        "def fit_firing_gain(stim_currents, spike_counts, spike_rates, to_plot=False):\n",
        "    'Gathers the firing rate of each stimuli and fits the linear portion of the curve to return the Gain in Hz/pA (the slope)'\n",
        "    is_pos_slope = np.diff(spike_counts,prepend=0)>0\n",
        "    is_pos_slope = movmean(np.diff(spike_counts,prepend=0),4)>0\n",
        "    peak_ind = np.where(spike_counts==np.max(spike_counts))[0]\n",
        "    if len(peak_ind)>1:\n",
        "        peak_ind = np.min(peak_ind)\n",
        "    \n",
        "    before_peak = np.arange(len(spike_counts))<peak_ind\n",
        "    is_nonzero = spike_counts>0\n",
        "    use_for_fit = np.logical_and.reduce((is_pos_slope,is_nonzero,before_peak))\n",
        "\n",
        "    if_fit = {}\n",
        "    if_fit['stim_currents'] = stim_currents\n",
        "    if_fit['spike_rates'] = spike_rates\n",
        "    if 0 == np.sum(spike_rates):\n",
        "        # print('no spikes detected')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if np.sum(spike_rates>0)<3:\n",
        "        # print('not enough spikes generated')\n",
        "        if_fit['slope'] = np.nan\n",
        "        if_fit['intercept'] = np.nan\n",
        "        if_fit['R2'] = 0\n",
        "        return if_fit\n",
        "\n",
        "    if_fit['slope'], if_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents[use_for_fit], spike_rates[use_for_fit])\n",
        "    if_fit['R2'] = r_value**2\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax = plt.subplots(1, figsize=[3,3])\n",
        "        ax.scatter( if_fit['stim_currents'] ,if_fit['spike_rates'] )\n",
        "        ax.plot( if_fit['stim_currents'], if_fit['slope']* if_fit['stim_currents']+if_fit['intercept'])\n",
        "        ax.scatter( if_fit['stim_currents'][use_for_fit] ,if_fit['spike_rates'][use_for_fit], color='r' )\n",
        "        ax.scatter(if_fit['stim_currents'][peak_ind],if_fit['spike_rates'][peak_ind], color='m')\n",
        "        ax.set_xlabel('current')\n",
        "        ax.set_ylabel('Spike Rate (Hz)')\n",
        "        (min,max) = ax.get_ylim()\n",
        "        ax.text(0, max/2, 'R**2='+str(round(if_fit['R2'],2)),fontsize='large')\n",
        "        plt.show()\n",
        "    return if_fit\n",
        "\n",
        "def check_inactivation( time, trace, is_stim, sample_rate, dVds, inds, mean_spike_rate, to_plot=0 ):\n",
        "    time_ms = time*1000\n",
        "    sum_isi = np.nan\n",
        "    rel_firing_duration = np.nan\n",
        "    if len(inds)>1:\n",
        "        isi = np.diff(time_ms[inds])\n",
        "        sum_isi = np.sum(isi)\n",
        "        stim_time = time_ms[np.where(is_stim)[0][0]]\n",
        "        first_time = time_ms[inds[0]]-stim_time\n",
        "        firing_duration = first_time+sum_isi\n",
        "        rel_firing_duration = firing_duration /(np.max(time[is_stim]*1000)-stim_time)\n",
        "    return rel_firing_duration\n",
        "\n",
        "\n",
        "\n",
        "# file_name = '2022-08-08_hipp_data/2022x08x04_NEL2_E4KI_F_P243_s001_c003_DGxNEG_0004.abf'\n",
        "# R2_thresh = 0\n",
        "# to_plot = 1\n",
        "\n",
        "# gain_slope, R2, stim_currents, spike_counts, v_before_stim, fire_dur = analyze_gain_abf(file_name,spike_args,R2_thresh,to_plot) \n",
        "# print(np.nanmedian(fire_dur))"
      ],
      "metadata": {
        "id": "dZQqWaElmaE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "########## Rheobase analysis ################\n",
        "def Rheo_curve_analysis(abf_recordings_df, protocol_aliases,to_plot = False,single_spike=True,verbose=False):\n",
        "    'Loop through abfs to calculate Rheobase. For each abf the first sweep with '\n",
        "    'a spike that occurs after a sweep without is chosen, and its stimulus amplitude'\n",
        "    'is written to the dataframe'\n",
        "    # return dataframe with updated min stim intensity\n",
        "    abf_recordings_df['Rheobase_(pA)'] = np.nan\n",
        "    abf_recordings_df['step_resolution_(pA)']  = np.nan\n",
        "    abf_recordings_df['Rheo Ihold_(pA)']  = np.nan\n",
        "    abf_recordings_df['Vhold_Rheo_(mV)']  = np.nan\n",
        "\n",
        "\n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        if verbose: print(file_name)\n",
        "        rheo, step_resolution, offset, Vhold_spike, ap_thresh = analyze_rheo(file_name,spike_args,to_plot=to_plot,verbose=verbose,single_spike=single_spike)\n",
        "\n",
        "        abf_recordings_df.at[file_name,'Rheobase_(pA)'] = rheo\n",
        "        abf_recordings_df.at[file_name,'step_resolution_(pA)'] = step_resolution\n",
        "        abf_recordings_df.at[file_name,'Rheo Ihold_(pA)'] = offset\n",
        "        abf_recordings_df.at[file_name,'Vhold_Rheo_(mV)'] = Vhold_spike\n",
        "        abf_recordings_df.at[file_name,'AP_Threshold(mV)'] = ap_thresh\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def analyze_rheo(file_name,spike_args,to_plot=False,verbose=False,single_spike=True):\n",
        "    abf = abf_or_name(file_name)\n",
        "    if to_plot: plot_sweeps_and_command(abf)\n",
        "    if len(abf.sweepList)<2:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan  \n",
        "    else:\n",
        "        is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "        stim_currents, spike_counts, spike_rates, V_before,_,_ = spikes_per_stim(abf, spike_args,thresh=20,to_plot=to_plot)\n",
        "        if verbose: print(spike_counts)\n",
        "        single_spikes = spike_counts==1\n",
        "        zero_spikes = spike_counts==0\n",
        "        if single_spike:\n",
        "            none_to_one = np.full(single_spikes.shape, False)\n",
        "            none_to_one[1:] = np.logical_and(single_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_one)[0]\n",
        "        else:\n",
        "            some_spikes = spike_counts>0\n",
        "            none_to_some = np.full(single_spikes.shape, False)\n",
        "            none_to_some[1:] = np.logical_and(some_spikes[1:], zero_spikes[:-1])\n",
        "            first_spike_stim = np.where(none_to_some)[0]\n",
        "    if first_spike_stim.size == 0:\n",
        "        return np.nan,np.nan,np.nan,np.nan,np.nan \n",
        "    else:\n",
        "        if first_spike_stim.size >1:\n",
        "            first_spike_stim = np.min(first_spike_stim)\n",
        "        rheo = stim_currents[first_spike_stim]\n",
        "        _, _, _, QC_val_df = Iclamp_QC(file_name)\n",
        "        offset = np.mean(QC_val_df['I_leak'])\n",
        "        Vhold_spike = QC_val_df['V_hold'][first_spike_stim]\n",
        "        ap_thresh = V_before[first_spike_stim]\n",
        "        step_resolution = np.mean(np.diff(stim_currents))\n",
        "\n",
        "    return rheo, step_resolution, offset, Vhold_spike, ap_thresh \n",
        "\n",
        "def spikes_per_stim(abf,spike_args,thresh=20,mode='count', to_plot=0):\n",
        "    'Loops through sweeps of and abf to find spikes'\n",
        "    # init\n",
        "    stim_currents = []\n",
        "    spike_rates = []\n",
        "    spike_counts = []\n",
        "    v_before_spike1 = []\n",
        "    v_before_stim = []\n",
        "    fire_dur = []\n",
        "    # get sweep info\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "\n",
        "    # get spike per sweep\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(abf.sweepY,abf.sampleRate,spike_args,is_stim=is_stim,mode='count',to_plot=to_plot)\n",
        "        rel_firing_duration = check_inactivation( abf.sweepX, abf.sweepY, is_stim, abf.sampleRate, dVds, inds, mean_spike_rate, to_plot=0 )\n",
        "        # plot id'd spikes\n",
        "        if to_plot>1:\n",
        "            fig, axs = plt.subplots(1)\n",
        "            axs.scatter(abf.sweepX[inds],abf.sweepY[inds],color='red',zorder=2)\n",
        "            axs.plot(abf.sweepX ,abf.sweepY,zorder=1)\n",
        "            plt.show()\n",
        "        # calc multi sweep params\n",
        "        stim_level = np.median(abf.sweepC[is_stim])\n",
        "        stim_currents.append(stim_level)\n",
        "        spike_rates.append(mean_spike_rate)\n",
        "        spike_counts.append(len(inds))\n",
        "        is_prestim = np.equal(np.cumsum( np.diff(is_base,prepend=1)),0)\n",
        "        v_before_stim.append( np.mean(abf.sweepY[is_prestim] ))\n",
        "        fire_dur.append(rel_firing_duration)\n",
        "\n",
        "        if len(inds)>0:\n",
        "            v_before_spike1.append(abf.sweepY[inds[0]])\n",
        "        else:\n",
        "            v_before_spike1.append(np.nan)\n",
        "\n",
        "    return np.array(stim_currents), np.array(spike_counts), np.array(spike_rates), np.array(v_before_spike1), np.array(v_before_stim) , np.array(fire_dur)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def find_spike_in_trace(trace,rate,spike_args,refract=0.005,is_stim = None ,mode='count',sanity_check=True,to_plot=0):\n",
        "    'Takes in a voltage trace from current clamp mode and uses derivative (dVds) to find action potentials.'\n",
        "    'Returns the dVds trace, boolean array indicating if dVds>threshold, inicies where dV crossed threshold,'\n",
        "    'and the mean firing rate given # spikes in trace of given length. Optional ways to count are:'\n",
        "    'isi (1/interspike interval) or count (spike count per second). Default is count'\n",
        "\n",
        "    high_dv_thresh = spike_args['high_dv_thresh']\n",
        "    low_dv_thresh = spike_args['low_dv_thresh']\n",
        "    spike_thresh = spike_args['spike_thresh']\n",
        "    window_ms = spike_args['window_ms']\n",
        "\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    dVds = np.diff(trace, prepend=trace[0])*rate/1000\n",
        "    over_thresh = dVds>spike_thresh\n",
        "    over_thresh[np.logical_not(is_stim)] = False\n",
        "    refract_window = int(np.round((refract*rate)))\n",
        "    inds = [t for t in np.arange(refract_window,len(over_thresh)) if all([over_thresh[t], all(over_thresh[t-refract_window:t]==False)])]    \n",
        "    if sanity_check:\n",
        "        old_inds = inds\n",
        "        inds = []\n",
        "        for i in old_inds:\n",
        "            samp_window = window_ms/1000 * rate\n",
        "            ind_range = np.arange(i-samp_window,i+samp_window).astype(int)\n",
        "            nearby_dVds = dVds[ind_range]\n",
        "            if False: print(i,'max', np.max(nearby_dVds))\n",
        "            if False: print(i,'min', np.min(nearby_dVds))\n",
        "            if np.max(nearby_dVds)>high_dv_thresh and np.min(nearby_dVds) < low_dv_thresh:\n",
        "                inds.append(i)\n",
        "                if False: print(inds)\n",
        "    if to_plot>2:\n",
        "        fig1, axs1 = plt.subplots(1,figsize = [9,2])\n",
        "        axs1.plot(np.arange(len(dVds))/rate,dVds,zorder=1)\n",
        "        axs1.scatter((np.arange(len(dVds))/rate)[inds],dVds[inds],color='red',zorder=2)\n",
        "        plt.show()\n",
        "    if len(inds)<1:\n",
        "        mean_spike_rate = 0\n",
        "    else:\n",
        "        if mode=='isi':\n",
        "            mean_spike_rate = np.mean(rate/np.diff(inds))\n",
        "        elif mode=='count':\n",
        "            mean_spike_rate = len(inds)/(np.sum(is_stim)/rate)\n",
        "        else:\n",
        "            print('invalid mode. using default (count)')\n",
        "    return dVds, over_thresh, inds, mean_spike_rate"
      ],
      "metadata": {
        "id": "WRo06YBZG3Ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Analyze R-Input In Current Clamp ##################################\n",
        "\n",
        "def inputR_analysis(abf_recordings_df, protocol_aliases,to_plot=False, verbose = False):\n",
        "    'Loop through abfs to calculate the input resistance in Current Clamp'\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if len(abf.sweepList)<2:\n",
        "            # print(list(abf_recordings_df.index).index(file_name),    'not enough sweeps')\n",
        "            abf_recordings_df.at[file_name,'Rinput_(MO)'] = np.nan\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "            \n",
        "        inputR_fit = input_res_curve(abf,to_plot=to_plot)\n",
        "        abf_recordings_df.at[file_name,'Rinput_(MO)'] = inputR_fit['slope']*1000\n",
        "        if len(abf_recordings_df.at[file_name,'passing_sweeps'])==0:\n",
        "            abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = np.nan\n",
        "            continue\n",
        "        Cmf_IC = IC_sweep_capacitance_mean(abf,abf_recordings_df.at[file_name,'passing_sweeps'],to_plot=to_plot,verbose=verbose)\n",
        "        abf_recordings_df.at[file_name,'Cmf_IC_(pF)'] = Cmf_IC\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def input_res_curve(abf,to_plot=False):\n",
        "    'Calulates the series of delta Vs and delta Is and fits with a line to find the resistance.'\n",
        "    from scipy.signal import butter,filtfilt\n",
        "    stim_currents = []\n",
        "    ss_voltage = []\n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s)\n",
        "        delta_v, _, _ = sweep_VIR(abf.sweepY, abf.sampleRate, is_stim = is_stim)\n",
        "        delta_I, _, _    = sweep_VIR(abf.sweepC, abf.sampleRate, is_stim = is_stim) # repurpose but for command current\n",
        "        stim_currents.append( delta_I)\n",
        "        ss_voltage.append(delta_v)\n",
        "    \n",
        "    inputR_fit = {}\n",
        "    inputR_fit['slope'], inputR_fit['intercept'] , r_value, p_value, std_err = stats.linregress(stim_currents, ss_voltage)\n",
        "    inputR_fit['R2'] = r_value**2\n",
        "    if to_plot:\n",
        "        plt.scatter(stim_currents, ss_voltage)\n",
        "        matplotlib.pyplot.text(10, 0, 'Ri = ' + str(round(inputR_fit['slope']*1000)) + 'MO')\n",
        "    return inputR_fit        \n",
        "\n",
        "def sweep_VIR(trace,rate,is_stim = None, window_t=0.100):\n",
        "    'Takes a trace snd calulates the steady state delta V from'\n",
        "    'a stimulus in Current Clamp'\n",
        "    if any(is_stim == None):\n",
        "        is_stim = [True for i in trace]\n",
        "    base_v = trace[:np.where(is_stim==True)[0][0]]\n",
        "    cutoff = 5\n",
        "    nyq = rate/2\n",
        "    normal_cutoff = cutoff / nyq\n",
        "    b, a = butter(3, normal_cutoff, btype='low')\n",
        "    filtered_step_v = filtfilt(b, a, trace[is_stim])\n",
        "    window_wid = int(window_t*rate)\n",
        "    med_base_v = np.median(base_v[-window_wid:-1])\n",
        "    med_stim_v = np.median(filtered_step_v[-window_wid:-1])\n",
        "    delta_v = med_stim_v - med_base_v\n",
        "    return delta_v, med_base_v, med_stim_v\n",
        "\n",
        "def binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss):\n",
        "    'A double exponential decay function'\n",
        "    return (amp_1 * np.exp(-time /tau_1)) + (amp_2 * np.exp(-time / tau_2))  + ss\n",
        "\n",
        "\n",
        "def IC_sweep_capacitance_mean(abf,passing_sweeps,to_plot=False, verbose = False):\n",
        "    'Takes an abf and calulates the membrane capacitance using'\n",
        "    'methods described in https://journals.physiology.org/doi/epdf/10.1152/jn.00160.2009'\n",
        "    \n",
        "    \n",
        "    is_base, is_stim = protocol_baseline_and_stim(abf)\n",
        "    rVm_list = []\n",
        "    V0_list = []\n",
        "    SS_list = []\n",
        "    Stim_list = []\n",
        "    time = abf.sweepX[is_stim]\n",
        "    time = time - time[0]\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        Istim = abf.sweepC[is_stim]\n",
        "        if not( np.mean(Istim) < 0 ):\n",
        "            continue\n",
        "        Vm = abf.sweepY[is_stim]\n",
        "        SS = np.percentile(Vm,.0001)\n",
        "        V0 = Vm[0]\n",
        "        relative_Vm = (Vm - SS) / (V0-SS)\n",
        "        rVm_list.append(relative_Vm)\n",
        "        V0_list.append(V0)\n",
        "        SS_list.append(SS)\n",
        "        Stim_list.append(np.median(Istim))\n",
        "    if len(V0_list)<1:\n",
        "        'likely there are passing sweeps but they are positive current'\n",
        "        return np.nan\n",
        "    \n",
        "    rVm_array = np.stack(rVm_list).T\n",
        "    V0_array = np.array(V0_list)\n",
        "    SS_array = np.array(SS_list)\n",
        "    Stim_array = np.array(Stim_list)\n",
        "    mean_rVm = np.mean(rVm_array,axis=-1)\n",
        "    if to_plot:\n",
        "        plt.plot(time,rVm_array,'gray')\n",
        "        plt.plot(time,mean_rVm,'k')\n",
        "\n",
        "\n",
        "    p0 = ( 0.1, 0.8, 0.0001,  0.01, 0)\n",
        "    bounds=([0,.3,0,.01,-.2], [0.25,1.2,.01,2,.2])\n",
        "\n",
        "    try:\n",
        "        fit_params, cv = scipy.optimize.curve_fit(binary_exp, time, mean_rVm, p0, bounds=bounds) #\n",
        "        amp_1, amp_2, tau_1, tau_2, ss = fit_params\n",
        "        Vm_hat = binary_exp(time, amp_1, amp_2, tau_1, tau_2, ss)\n",
        "\n",
        "        Vm_hat1 = binary_exp(time, amp_1, 0, tau_1, tau_2, ss) + amp_2\n",
        "        Vm_hat2 = binary_exp(time, 0, amp_2,  tau_1, tau_2, ss) + amp_1\n",
        "\n",
        "        t_slow = tau_2*np.ones_like(V0_array)\n",
        "        dv_slow = (SS_array - V0_array) * amp_2 *1e-3\n",
        "        i_stim = Stim_array*1e-12\n",
        "        r_slow = dv_slow/i_stim\n",
        "        c_slow = t_slow /r_slow\n",
        "        Cmf_IC = c_slow*1e12\n",
        "    except:\n",
        "        print(V0_array)\n",
        "        print(Stim_array)\n",
        "        print(SS_array)\n",
        "        return np.nan \n",
        "\n",
        "    if verbose:\n",
        "        print('V0', V0_array)\n",
        "        print('I', i_stim*1e12)\n",
        "        print('dV', dv_slow*1e3)\n",
        "        print('SSv', SS_array)\n",
        "        print('Rm', r_slow*1e-6)\n",
        "        print('Cm', Cmf_IC)\n",
        "        print('amp_1, amp_2, tau_1, tau_2, ss')\n",
        "        print('Fit', fit_params)\n",
        "\n",
        "    if to_plot:\n",
        "        plt.plot(time,Vm_hat,'r')\n",
        "        plt.plot(time,Vm_hat1,'c')\n",
        "        plt.plot(time,Vm_hat2,'m')\n",
        "        plt.show()\n",
        "\n",
        "    return np.mean(Cmf_IC)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "TiU-YFY-G0kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################################  Quality Control Filtering ##################################\n",
        "def QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=[],IC_prot=[],MT_prot=[]):\n",
        "    'Loop through all abfs to look for signs of a poor recording including leak current, unstable holding V/I,'\n",
        "    'high noise, and in appropriate holidng potential'\n",
        "\n",
        "    # abf_recordings_df['QC_checks'] = None\n",
        "    # abf_recordings_df['QC_values'] = None\n",
        "    abf_recordings_df['passing_sweeps'] = None\n",
        "\n",
        "    print('Voltage Clamp Protocols')\n",
        "    VC_idx = [p in VC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[VC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "\n",
        "    print('Current Clamp Protocols')\n",
        "    IC_idx = [p in IC_prot for p in abf_recordings_df['protocol']]\n",
        "    for fn in tqdm( abf_recordings_df[IC_idx].index ):\n",
        "        if verbose: print(np.where(fn==abf_recordings_df.index)[0])\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Iclamp_QC(fn,to_plot=to_plot,verbose=verbose)\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = np.array(passing_sweeps)\n",
        "        except AssertionError:\n",
        "            if verbose: print_assert()\n",
        "            abf_recordings_df.at[fn,'passing_sweeps'] = []\n",
        "       \n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "def Vclamp_QC(file_name, max_leak=200,max_high_freq_noise = 10, max_low_freq_noise = 15,\n",
        "              Vhold_range = 8, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Voltage Clamp recordings'\n",
        "    is_VC = np.nan\n",
        "    is_Ic = np.nan\n",
        "    abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "\n",
        "\n",
        "    try:\n",
        "        assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "        return 0, [], [], []\n",
        "\n",
        "\n",
        "\n",
        "    # assert is_VC==True, 'Wrong clamp mode for protocol. Voltage protocol used during current clamp!'\n",
        "    \n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "\n",
        "\n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,abf.sampleRate,                                                 \n",
        "                                                 max_leak=max_leak,\n",
        "                                                 max_high_freq_noise = max_high_freq_noise,\n",
        "                                                 max_low_freq_noise = max_low_freq_noise,\n",
        "                                                 Vhold_range = Vhold_range, to_plot=False)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    if verbose: print('\\n','pass_rate:',pass_rate)\n",
        "    if verbose: print('passing_sweeps:',passing_sweeps)\n",
        "    return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def Iclamp_QC(file_name, max_leak=250, to_plot=False,verbose=False):\n",
        "    'Look for signs of a poor recording for Current Clamp recordings'\n",
        "    abf = abf_or_name(file_name)\n",
        "    # abf = pyabf.ABF( file_name )\n",
        "    if 'mV' in abf.sweepLabelY:\n",
        "        is_IC = True\n",
        "        is_VC = False\n",
        "    if 'pA' in abf.sweepLabelY:\n",
        "        is_IC = False\n",
        "        is_VC = True\n",
        "    try:\n",
        "        assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "    except AssertionError as e:\n",
        "        print(e)\n",
        "\n",
        "    # assert is_IC==True, 'Wrong clamp mode for protocol. IC protocol used during voltage clamp!'\n",
        "\n",
        "    theta, command_offset, correct_ch1 =  predict_telegraph(abf)\n",
        "    \n",
        "    QC_check_list = []\n",
        "    QC_val_list = []\n",
        "    for s in abf.sweepList:\n",
        "        abf.setSweep(s,0)\n",
        "        QC_checks, QC_values, windows = qc_sweep(abf.sweepX,abf.sweepC,abf.sweepY,command_offset,is_IC,is_VC,\n",
        "                                                 abf.sampleRate,to_plot=to_plot, max_high_freq_noise = .05, max_low_freq_noise = 0.4, Vhold_range=3)\n",
        "        QC_check_list.append(QC_checks)\n",
        "        QC_val_list.append(QC_values)\n",
        "    \n",
        "    QC_check_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_check_list)):\n",
        "        d = QC_check_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_check_df.at[i,k]=v\n",
        "\n",
        "    QC_val_df = pd.DataFrame({'sweep' : np.arange(len(QC_val_list))}).set_index('sweep')\n",
        "    for i in np.arange(len(QC_val_list)):\n",
        "        d = QC_val_list[i]\n",
        "        for (k,v) in d.items():\n",
        "            QC_val_df.at[i,k]=v\n",
        "\n",
        "    pass_rate = {}\n",
        "    mean_values = {}\n",
        "    for c in QC_check_df.columns:\n",
        "        pass_rate[c] = np.round(np.mean(np.array(QC_check_df[c].values).astype(int))*100,2)\n",
        "        mean_values[c] = np.round(np.mean(np.array(QC_val_df[c].values)),3)\n",
        "    passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,:])]\n",
        "    \n",
        "    if verbose==True: \n",
        "        print('')\n",
        "        print('pass_rate:',pass_rate)\n",
        "        print('mean_values:',mean_values)\n",
        "    if verbose==2:\n",
        "        print('\\n','pass_rate:',pass_rate)\n",
        "        print('passing_sweeps:',passing_sweeps, str(len(passing_sweeps)/len(QC_check_df)*100)+'%')\n",
        "        print(QC_val_df)\n",
        "\n",
        "    if to_plot:\n",
        "        fig, ax, theta = plot_sweeps_and_command(abf,windows=windows)\n",
        "        plt.show()\n",
        "\n",
        "    return [pass_rate,passing_sweeps,QC_check_df,QC_val_df] # return pass_rate, passing_sweeps, QC_check_df, QC_val_df\n",
        "\n",
        "def qc_sweep(sweepX,sweepC,sweepY,command_offset,is_IC,is_VC,sampleRate,\n",
        "             max_leak=100, \n",
        "             max_high_freq_noise = 10,\n",
        "             max_low_freq_noise = 10,\n",
        "             Vhold_range = 5, to_plot=False):\n",
        "    'Recieves a sweep and and calculates leak, noise and holding potential'\n",
        "    'returns a dict of calculated values and a dict of boolean indicating'\n",
        "    'pass/fail, (True/False)'\n",
        "\n",
        "\n",
        "    stim_buffer_time = 250 #ms\n",
        "    filtered_command = movmean((sweepC==sweepC[0])*1, stim_buffer_time/1000*sampleRate)\n",
        "    ss_no_stim_bool = filtered_command==1\n",
        "    ss_no_stim_idx = np.arange(len(ss_no_stim_bool))[ss_no_stim_bool]\n",
        "    no_stim_sig = sweepY[ss_no_stim_bool]\n",
        "    no_stim_t = sweepX[ss_no_stim_idx]\n",
        "    baseline = np.mean( no_stim_sig )\n",
        "\n",
        "    QC_checks = {}\n",
        "    QC_values = {}\n",
        "    if is_IC:\n",
        "        QC_values['V_hold'] = baseline\n",
        "        QC_values['I_leak'] = command_offset\n",
        "    if is_VC:\n",
        "        QC_values['V_hold'] = command_offset\n",
        "        QC_values['I_leak'] = baseline\n",
        "    \n",
        "    QC_checks['V_hold'] = abs(QC_values['V_hold'] - -70)< Vhold_range\n",
        "    QC_checks['I_leak'] = QC_values['I_leak']<max_leak\n",
        "\n",
        "\n",
        "    \n",
        "    HF_noise_idx = ss_no_stim_idx[:int(0.0015*sampleRate)]\n",
        "    HF_noise_signal = sweepY[ HF_noise_idx ] \n",
        "    HF_noise = rms_noise(HF_noise_signal)     \n",
        "    \n",
        "    QC_checks['HF_noise'] = HF_noise<max_high_freq_noise\n",
        "    QC_values['HF_noise'] = HF_noise\n",
        "    \n",
        "    LF_noise_idx = ss_no_stim_idx[-int(0.1*sampleRate):] \n",
        "    if int(0.1*sampleRate)>len(LF_noise_idx): LF_noise_idx = np.random.choice(LF_noise_idx, size=int(0.1*sampleRate))\n",
        "    LF_noise_signal = sweepY[ LF_noise_idx ] \n",
        "    LF_noise = rms_noise(LF_noise_signal)\n",
        "    QC_checks['LF_noise'] = LF_noise<max_low_freq_noise\n",
        "    QC_values['LF_noise'] = LF_noise\n",
        "\n",
        "    HF_noise_time = sweepX[HF_noise_idx]\n",
        "    LF_noise_time = sweepX[LF_noise_idx]\n",
        "    LF_noise_time = np.sort(np.array(list(set(LF_noise_time))))\n",
        "\n",
        "    return QC_checks, QC_values, [HF_noise_time, LF_noise_time]\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x46MR5nvGrV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "################################## Membrane Resistance & Capacitance Testing ##################################\n",
        "def Icapacitance_analysis(abf_recordings_df, protocol_aliases, to_plot=False,verbose=False,report_params=True):\n",
        "    'Loops through abfs with Vclamp pulstrains to calculate membrane properties: Ra, Rm, Cm'\n",
        "    if report_params:\n",
        "        report_params = ['Ra', 'Rm', 'Cm', 'tau',\t'Cmq',\t'Cmf',\t'Cmqf', 'Cm_pc']\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    print(len(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol] ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        try:\n",
        "            pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=verbose)\n",
        "            passing_sweeps = [s for s in QC_check_df.index if all(QC_check_df.loc[s,['I_leak','HF_noise','LF_noise']])] # ignore Vhold Filters\n",
        "            mem_params_df = fit_Icapacitave_mean_current(abf,to_plot=to_plot,verbose=verbose,passing_sweeps=passing_sweeps)\n",
        "            pclamp_mem_params_df = pclamp_mem_test(abf,to_plot = to_plot, verbose = verbose)\n",
        "            mem_params_df = mem_params_df.join(pclamp_mem_params_df,how='outer')\n",
        "\n",
        "\n",
        "        except:\n",
        "            _=print_assert()\n",
        "            mem_params_df = pd.DataFrame()\n",
        "        for c in mem_params_df.columns:\n",
        "            for d in mem_params_df.index:\n",
        "                if c in report_params:\n",
        "                    abf_recordings_df.at[file_name, c+'_'+str(d)] = mem_params_df.at[d,c]\n",
        "    return abf_recordings_df, correct_protocol\n",
        "       \n",
        "\n",
        "def fit_Icapacitave_mean_current(abf, to_plot=False, verbose=False, passing_sweeps = []):\n",
        "    'Takes in an abf file and finds all pulses. Pulses with matching duration are averaged together.'\n",
        "    'For each pulse duration the mean pulse is fit using the methods described at https://swharden.com/blog/2020-10-11-model-neuron-ltspice/ '\n",
        "    'For each pulse length returns, Ra, Rm, and three Cm measures (Cmf, Cmq, Cmqf).'\n",
        "    'Respectively these are capacitance determined by: fitting tau and computing,'\n",
        "    'calculating the area under the capcitave transient, and calculating the area'\n",
        "    'under the fit line.'\n",
        "\n",
        "    command = abf.sweepC\n",
        "    # trace,time,command,rate,\n",
        "\n",
        "    base_v = command[0]\n",
        "    step_v = np.median( command[np.logical_not(command==base_v)])\n",
        "    is_base = command==base_v\n",
        "    is_step = command==step_v\n",
        "\n",
        "    delta_V = abs(step_v-base_v)\n",
        "\n",
        "    step_start = np.logical_and(is_base[:-1], is_step[1:])\n",
        "    step_stop = np.logical_and(is_step[:-1], is_base[1:])\n",
        "\n",
        "    starts = np.where(step_start)[0]\n",
        "    stops = np.where(step_stop)[0]\n",
        "\n",
        "    assert len(starts)==len(stops), 'unable to match pulse starts and stops'\n",
        "    assert any(( starts-stops)<0), 'unable to match pulse starts and stops'\n",
        "    assert len(starts)>0, 'no pulse found'\n",
        "    # parse_pulses\n",
        "\n",
        "    if verbose: print('passing_sweeps',passing_sweeps)\n",
        "\n",
        "    params = []\n",
        "    p_len_list = []\n",
        "    Icap_list = []\n",
        "    step_time_list = []\n",
        "    # for s in abf.sweepList:\n",
        "    for s in passing_sweeps:\n",
        "        abf.setSweep(s)\n",
        "        trace = abf.sweepY\n",
        "        sweep_time = abf.sweepX\n",
        "        if (base_v>step_v):\n",
        "            trace = -trace\n",
        "        for p in np.arange(len(starts)):\n",
        "            pulse_start = starts[p]\n",
        "            pulse_stop = stops[p]\n",
        "            pulse_len = stops[p] - starts[p]\n",
        "            p_len_list.append(pulse_len)\n",
        "            pulse_index = np.arange(int(pulse_start-pulse_len*0.05),pulse_stop)\n",
        "\n",
        "            step_times = sweep_time[pulse_index]\n",
        "            step_times = step_times-sweep_time[starts[p]]\n",
        "            step_time_list.append(step_times)\n",
        "\n",
        "            Icap_transient = trace[pulse_index]\n",
        "            Icap_list.append(Icap_transient)\n",
        "\n",
        "    p_len_list = np.array(p_len_list)/abf.sampleRate*1000\n",
        "    pulse_set = np.array(sorted(set(p_len_list)))\n",
        "    mem_params_df = pd.DataFrame(None,index=pulse_set,columns=['>90%','Ib','Iss','Ip','Ra','Rm','tau','Cmq','Cmf','Cmqf'])\n",
        "    \n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,len(pulse_set),figsize=[12, 3])\n",
        "        fig.suptitle(abf.abfFilePath)\n",
        "        if verbose: print(abf.abfFilePath)\n",
        "        if str(type(axs)) == \"<class 'matplotlib.axes._subplots.AxesSubplot'>\":\n",
        "            axs = [axs]\n",
        "    \n",
        "    \n",
        "    for p in pulse_set:\n",
        "        # pulse_dur =p/abf.sampleRate*1000\n",
        "        matching_traces = [Icap_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]\n",
        "        matching_traces = np.stack(matching_traces)\n",
        "\n",
        "        mean_trace = np.mean(matching_traces,axis=0)\n",
        "        mean_time = np.mean(np.stack([step_time_list[n] for n in np.arange(len(p_len_list)) if p_len_list[n]==p ]),axis=0)\n",
        "\n",
        "        sweep_var = abs((matching_traces-mean_trace)/mean_trace)\n",
        "        outlier_percent = round(np.mean(sweep_var>1.645)*100,3)\n",
        "        # base_ind = np.arange(len(mean_time))\n",
        "        base_t = np.mean(mean_time[mean_time<0])\n",
        "        base_I = np.mean(mean_trace[mean_time<0])\n",
        "        \n",
        "        steady_state_t = np.mean(mean_time[mean_time>mean_time[-1]*0.95])\n",
        "        steady_state_I = np.mean(mean_trace[mean_time>mean_time[-1]*0.95])\n",
        "\n",
        "\n",
        "        peak_I = np.max(mean_trace)\n",
        "        peak_t = mean_time[mean_trace==peak_I]\n",
        "        if peak_t.shape[0]>1: peak_t = min(peak_t)\n",
        "        Icap_curve = (mean_trace[mean_time>=peak_t])\n",
        "        Icap_curve_t = mean_time[mean_time>=peak_t]\n",
        "\n",
        "\n",
        "        rel_dif_Icap = movmean(np.diff(Icap_curve,append=Icap_curve[-1]),10)/peak_I\n",
        "        excess_plat_t = Icap_curve_t[rel_dif_Icap>=0]\n",
        "        if len(excess_plat_t)>0:\n",
        "            excess_plat_start = np.min(excess_plat_t)*10\n",
        "            if excess_plat_start >0.005:\n",
        "                Icap_curve = Icap_curve[Icap_curve_t<excess_plat_start]\n",
        "                Icap_curve_t = Icap_curve_t[Icap_curve_t<excess_plat_start]\n",
        "                steady_state_t = np.mean(Icap_curve_t[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                steady_state_I = np.mean(Icap_curve[Icap_curve_t>Icap_curve_t[-1]*0.95])\n",
        "                # plt.scatter(Icap_curve_t,Icap_curve)\n",
        "                # plt.scatter(excess_plat_t,Icap_curve[rel_dif_Icap>=0])\n",
        "                # plt.gca().set_xscale('log')\n",
        "                # plt.scatter(excess_plat_start,peak_I)\n",
        "                # plt.show()\n",
        "\n",
        "\n",
        "        \n",
        "        delta_I_steady = steady_state_I - base_I\n",
        "        delta_I_peak = peak_I - steady_state_I\n",
        "        # if verbose: print('len(Icap_curve)',len(Icap_curve))\n",
        "        # if verbose: print('delta_V',delta_V)\n",
        "        # if verbose: print('delta_I_peak',delta_I_peak)\n",
        "        Ra = (delta_V*1e-3)/(delta_I_peak*1e-12) *1e-6 #(O/MO)\n",
        "        Rm = ((delta_V*1e-3) - Ra*1e6 * delta_I_steady*1e-12) / (delta_I_steady*1e-12) *1e-6 #(O/MO)\n",
        "        Q = np.sum(Icap_curve-steady_state_I) * (1/abf.sampleRate)\n",
        "        Cmq = Q / delta_V*1000\n",
        "        # if verbose: print('Cmq',Cmq)\n",
        "        \n",
        "\n",
        "        try:\n",
        "            bounds=([peak_I*0.1,.0001,0], [peak_I*1.5,500, steady_state_I*3])\n",
        "            p0 = (peak_I, 0.02 , steady_state_I) # start with values near those we expect\n",
        "            fit_params, cv = scipy.optimize.curve_fit(mono_exp, Icap_curve_t[int(0.0005*abf.sampleRate):], Icap_curve[int(0.0005*abf.sampleRate):], p0, bounds=bounds) #\n",
        "            peak_hat, tau_hat, ss_hat = fit_params\n",
        "            Icap_hat =  mono_exp(Icap_curve_t, peak_hat, tau_hat, ss_hat)\n",
        "            perr = np.sqrt(np.diag(cv))\n",
        "            # if verbose: print('tau_hat',tau_hat)\n",
        "            # if verbose: print('Icap_curve_t',Icap_curve_t)\n",
        "            # if verbose: print('Ra',Ra)\n",
        "            # if verbose: print('Rm',Rm)\n",
        "            Cmf = tau_hat / (1/(1/(Ra*1e6) + 1/(Rm*1e6)))\n",
        "            Cmf = Cmf*1e12\n",
        "            # if verbose: print('Cmf',Cmf)\n",
        "            \n",
        "        except:\n",
        "            Cmf = None\n",
        "            Icap_hat = np.empty_like(Icap_curve_t)\n",
        "            Icap_hat[:] =np.nan\n",
        "            ss_hat = np.nan\n",
        "            tau_hat = np.nan\n",
        "\n",
        "        Cmqf = np.sum(Icap_hat-ss_hat) * (1/abf.sampleRate) / delta_V*1000\n",
        "        # if verbose: print('Cmqf',Cmqf)\n",
        "\n",
        "        mem_params_df.at[p] = [outlier_percent,base_I,steady_state_I,peak_I,Ra,Rm,tau_hat,Cmq,Cmf,Cmqf]\n",
        "        \n",
        "\n",
        "        if to_plot:\n",
        "            i = int(np.where(p==pulse_set)[0][0])\n",
        "            mean_time_0 = -mean_time[0]\n",
        "            axs[i].plot(mean_time_0+mean_time,matching_traces.T,color = (0.8,0.8,0.8))\n",
        "            axs[i].plot(mean_time_0+mean_time,mean_trace,color='k')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],base_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].scatter(mean_time_0+peak_t,peak_I,color='r',zorder=5)\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[[0,-1]],steady_state_I*np.array([1,1]),color='r',linestyle = 'dotted')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t[int(0.001*abf.sampleRate):],Icap_curve[int(0.001*abf.sampleRate):],color='m')\n",
        "            axs[i].plot(mean_time_0+Icap_curve_t, Icap_hat,'c',linestyle = 'dashed')\n",
        "            # if verbose: print(steady_state_I)\n",
        "            # axs[i].plot(mean_time_0+Icap_curve_t, np.cumsum(Icap_hat-ss_hat)/3)\n",
        "            # axs[i].set_xscale('log')\n",
        "            axs[i].set_xlim([0,mean_time_0+Icap_curve_t[-1]*1.2]) #(mean_time_0+peak_t)*0.7\n",
        "            axs[i].set_title(str(p)+'ms')\n",
        "            \n",
        "    if verbose: display(mem_params_df)\n",
        "    if to_plot:\n",
        "        plt.tight_layout()\n",
        "        fig.subplots_adjust(top=0.8)\n",
        "        plt.show()          \n",
        "    return mem_params_df\n",
        "\n",
        "def pclamp_mem_test(abf,to_plot = False, verbose =False):\n",
        "    # load file if name given instead of true abf\n",
        "    abf = abf_or_name(abf)\n",
        "    command = abf.sweepC*1e-3\n",
        "    trace = abf.sweepY*1e-12\n",
        "    sweep_time = abf.sweepX\n",
        "\n",
        "    # make all pos\n",
        "    if np.mean(command)<0:\n",
        "        command = -command\n",
        "        trace = -trace\n",
        "        \n",
        "    \n",
        "\n",
        "    # Find step and recovery\n",
        "    base_v = command[0]\n",
        "    # plt.plot(sweep_time,command)\n",
        "    step_v = np.median( command[np.logical_not(command==base_v)])\n",
        "    dvdt = np.diff(command,prepend=command[0])\n",
        "    # plt.plot(sweep_time,dvdt)\n",
        "    up_step = np.where(dvdt==np.max(dvdt))[0]\n",
        "    # print('up_step',up_step)\n",
        "    down_step = np.where(dvdt==np.min(dvdt))[0]\n",
        "    # print('down_step',down_step)\n",
        "    updn_ticks = down_step - up_step\n",
        "    # print('updn_ticks',updn_ticks)\n",
        "\n",
        "    pulse_dur_set = np.sort(list(set(updn_ticks)))\n",
        "    # print('pulse_dur_set',pulse_dur_set)\n",
        "\n",
        "\n",
        "    mem_params_df = pd.DataFrame(None,index=pulse_dur_set/abf.sampleRate*1000,columns=['Tau_pc','Rm_pc','Ra_pc','Cm_pc'])\n",
        "\n",
        "    'For Each Pulse Duration Length'\n",
        "    for p in pulse_dur_set:\n",
        "        'Average up the pulses'\n",
        "        matching_starts = [up_step[i] for i in range(len(up_step)) if updn_ticks[i]==p ]\n",
        "        tick_range = np.arange(p*2)\n",
        "        pulse_indicies_mat = np.add.outer(matching_starts,tick_range)\n",
        "        pulse_trace_set = trace[pulse_indicies_mat]\n",
        "        mean_pulse_trace = np.mean(pulse_trace_set,axis = 0)\n",
        "        mean_pulse_command = np.mean(command[pulse_indicies_mat],axis = 0)\n",
        "        if to_plot:\n",
        "            fig, ax = plt.subplots(1)\n",
        "            ax.plot(pulse_trace_set.transpose(),color='grey')\n",
        "            ax.plot(mean_pulse_trace,color='k')\n",
        "            # plt.show()\n",
        "        'Get pclamp fitting variables'\n",
        "        'Get Is and Vs'\n",
        "        V1 = np.max(mean_pulse_command)\n",
        "        V2 = np.min(mean_pulse_command)\n",
        "        delta_V = V1-V2\n",
        "        I1_index = range(int(p*0.8),p)\n",
        "        I1 = np.mean(mean_pulse_trace[I1_index])\n",
        "\n",
        "        I2_index = I1_index + p\n",
        "        I2 = np.mean(mean_pulse_trace[I2_index])\n",
        "        delta_I = I1-I2\n",
        "\n",
        "        if to_plot:\n",
        "            ax.plot(I1_index,I1*np.ones_like(I1_index),color='magenta',linewidth=3)\n",
        "            ax.plot(I2_index,I2*np.ones_like(I1_index),color='magenta',linewidth=3)\n",
        "            \n",
        "\n",
        "        'Fitting Tau'\n",
        "        def linearized_exp_decay(time,tau,beta):\n",
        "            'Linear form of ln(y) for exponential decay'\n",
        "            return -(1/tau)*(time+ beta) \n",
        "        \n",
        "        peak_I = np.max(mean_pulse_trace)\n",
        "        ind_of_peak = np.where(mean_pulse_trace==peak_I)[0]\n",
        "\n",
        "\n",
        "        single_pulse_trace = mean_pulse_trace[np.arange(ind_of_peak,p)]\n",
        "        single_pulse_time = np.arange(ind_of_peak,p)/abf.sampleRate\n",
        "        fraction_to_fit = [0.20, 0.80]\n",
        "\n",
        "        'LinearFraction'\n",
        "        I_max = np.max(single_pulse_trace)\n",
        "        I_min = np.min(single_pulse_trace)\n",
        "        delta = I_max - I_min\n",
        "        lower = I_min + delta*fraction_to_fit[0]\n",
        "        upper = I_min + delta*fraction_to_fit[1]\n",
        "        trimmed_fit_range = np.logical_and(single_pulse_trace>lower, single_pulse_trace<upper)\n",
        "        trace_to_fit = single_pulse_trace[trimmed_fit_range]\n",
        "        time_to_fit = single_pulse_time[trimmed_fit_range]\n",
        "        if to_plot:\n",
        "            ax.plot(time_to_fit*abf.sampleRate,trace_to_fit,color='green',linewidth=3)\n",
        "\n",
        "        'linear fit of ln_trace'\n",
        "        '(with baseline shift to avoid log(x<0)'\n",
        "        trace_to_fit = trace_to_fit\n",
        "        shift = abs(np.min(trace_to_fit))\n",
        "        ln_trace = np.log(trace_to_fit+shift*2)\n",
        "\n",
        "        \n",
        "\n",
        "        [tau_hat, beta_hat], cv = scipy.optimize.curve_fit(linearized_exp_decay, time_to_fit, ln_trace) #\n",
        "        \n",
        "\n",
        "\n",
        "        I_hat = linearized_exp_decay(time_to_fit,tau_hat, beta_hat)\n",
        "        if to_plot:\n",
        "            ax.plot(time_to_fit*abf.sampleRate,np.exp(I_hat)-shift*2,color='turquoise',linewidth=3)\n",
        "        \n",
        "        'Calculate Pclamp Values'\n",
        "        delta_I = I1-I2\n",
        "        Q2 = delta_I * tau_hat # This doesnt make sense to me\n",
        "        I_ss = np.mean([I1,I2])\n",
        "\n",
        "        Q1_ind = np.where(single_pulse_trace>I1)[0]\n",
        "        Q1 = np.sum(single_pulse_trace[Q1_ind] - I1) / abf.sampleRate\n",
        "        \n",
        "        'Plot Area'\n",
        "        patch_points = np.ones([len(Q1_ind)*2,2])\n",
        "        if to_plot:\n",
        "            patch_points[:,0] = np.concatenate((Q1_ind,np.flip(Q1_ind)))\n",
        "            patch_points[:,1] = np.concatenate((single_pulse_trace[Q1_ind],I1*np.ones_like(Q1_ind)))\n",
        "            poly = matplotlib.patches.Polygon(patch_points, color='orange')\n",
        "            ax.add_patch(poly)\n",
        "\n",
        "        'Calculate Pclamp Values'\n",
        "        Qt = Q1 + Q2\n",
        "        Cm = Qt / delta_V\n",
        "        Rt = delta_V/delta_I\n",
        "\n",
        "        'Iterateively Solve Ra using Newton-Raphson Method'\n",
        "        Ra_guess = 20*1e6\n",
        "        delta_guess = 1e10\n",
        "        tol = 1\n",
        "        while delta_guess>tol:\n",
        "            f_of_guess = Ra_guess**2 - Ra_guess*Rt + Rt*(tau_hat/Cm)\n",
        "            f_prime_of_guess = Ra_guess/2 - Rt\n",
        "            Ra_guess_new = Ra_guess - (f_of_guess/f_prime_of_guess)\n",
        "            delta_guess = Ra_guess_new - Ra_guess\n",
        "            Ra_guess = Ra_guess_new\n",
        "        Ra = Ra_guess\n",
        "        Rm = Rt - Ra\n",
        "\n",
        "\n",
        "        if verbose:\n",
        "            print('tau_hat',tau_hat*1000,'ms')\n",
        "            print('Cm',Cm*1e12,'pF')\n",
        "            print('Rt',Rt*1e-6,'MO')\n",
        "            print('Ra',Ra*1e-6,'MO')\n",
        "            print('Rm',Rm*1e-6,'MO')\n",
        "\n",
        "\n",
        "        'Return a dataframe of parameters'\n",
        "        p_ms = int(p/abf.sampleRate*1000)\n",
        "        mem_params_df.at[p_ms,'Tau_pc'] = tau_hat\n",
        "        mem_params_df.at[p_ms,'Rm_pc'] = Rm*1e-6\n",
        "        mem_params_df.at[p_ms,'Ra_pc'] = Ra*1e-6\n",
        "        mem_params_df.at[p_ms,'Cm_pc'] = Cm*1e12\n",
        "\n",
        "        if to_plot: plt.show()\n",
        "\n",
        "    return mem_params_df\n"
      ],
      "metadata": {
        "id": "Ne5WoYGnGmhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "####################### SPIKE LATENCY #######################\n",
        "\n",
        "def Spike_latency(abf_recordings_df, protocol_aliases,to_plot=False):\n",
        "    'Loops through abfs and calcualtes the time to first action potential'\n",
        "    'during a ramp current stimulation'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    # print(np.sum(correct_protocol),'files to analyze...')\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]): #tqdm( ) : \n",
        "        abf = pyabf.ABF( file_name )\n",
        "        # pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False)\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "        latencey_list = []\n",
        "        v_hold_list = []\n",
        "        \n",
        "\n",
        "        for s in abf.sweepList:\n",
        "            abf.setSweep(s)\n",
        "            # plot_sweeps_and_command(abf)\n",
        "            latencey, v_hold = analyze_ramp_sweep(abf.sweepX,abf.sweepY,abf.sweepC,\n",
        "                                              abf.sampleRate,to_plot=to_plot)\n",
        "            latencey_list.append(latencey)\n",
        "            v_hold_list.append(v_hold)\n",
        "        latencey_list =np.array(latencey_list)\n",
        "        v_hold_list =np.array(v_hold_list)\n",
        "        # if len(passing_sweeps)==0:\n",
        "        #     abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.nan\n",
        "        #     abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.nan\n",
        "        # else:      \n",
        "        # #     latencey_list = latencey_list[passing_sweeps]\n",
        "        # #     v_hold_list = v_hold_list[passing_sweeps]\n",
        "        abf_recordings_df.at[file_name,'Spike_Latency_(ms)'] = np.median(latencey_list)\n",
        "        abf_recordings_df.at[file_name,'V_hold_(Latency)'] = np.median(v_hold_list)\n",
        "    return abf_recordings_df\n",
        "        \n",
        "        \n",
        "def analyze_ramp_sweep(sweepX,sweepY,sweepC,rate,to_plot=False):\n",
        "    'Receives sweep data and finds the first AP and returns it.'\n",
        "    'Also retuns Vhold for quality control.'\n",
        "    is_base = sweepC==sweepC[0]\n",
        "    is_stim = np.logical_not(sweepC==sweepC[0])\n",
        "    ramp_start_ind = np.min(np.where(is_base==False))\n",
        "    v_hold = np.mean( sweepY[0:ramp_start_ind])\n",
        "    # print(sweepX,sweepY)\n",
        "\n",
        "    spike_args = {'spike_thresh':10,\n",
        "                    'high_dv_thresh': 25,\n",
        "                    'low_dv_thresh': -5,\n",
        "                     'window_ms': 2}\n",
        "\n",
        "    dVds, over_thresh, inds, mean_spike_rate = find_spike_in_trace(sweepY, rate,spike_args,is_stim=is_stim)\n",
        " \n",
        "    if len(inds)==0:\n",
        "        # print('no spikes found')\n",
        "        return np.nan,v_hold\n",
        "    latencey = sweepX[np.min(inds)-ramp_start_ind]*1000\n",
        "    if to_plot:\n",
        "        # plt.scatter(sweepX,dVds,color='k')\n",
        "        plt.plot(sweepX,sweepY,color='k')\n",
        "        plt.scatter(sweepX[inds],sweepY[inds],color='r' )\n",
        "        zoom_x_relativ = np.array([ 0.75, 1.5])\n",
        "        zoom_x = zoom_x_relativ*(latencey/1000+sweepX[ramp_start_ind])\n",
        "        # print('zoom_x',zoom_x)\n",
        "        # print('latencey',latencey)\n",
        "        # print('sweepX[ramp_start_ind]',sweepX[ramp_start_ind])\n",
        "        # print('ramp_start_ind',ramp_start_ind)\n",
        "        plt.gca().set_xlim(zoom_x)\n",
        "        plt.show()\n",
        "\n",
        "    return latencey, v_hold\n"
      ],
      "metadata": {
        "id": "6nDDntRwGZ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### Combine memtest measures ##########################\n",
        "def combine_memtest_durations(column_pairs,abf_recordings_df):\n",
        "    'For the niche case where membrane parameter calculated from'\n",
        "    'two pulse durations need to be combined, take a list of columns'\n",
        "    'to combine as a list of tuples, and creates a new column tagged'\n",
        "    '_Combo which consolidates the columns pairs.'\n",
        "\n",
        "    print('Combine long MemTest pulses...')\n",
        "    for p in tqdm(column_pairs) :\n",
        "        col_name = p[0][:p[0].index('_')]+'_Combo'\n",
        "        abf_recordings_df[col_name] = None\n",
        "        for rec in abf_recordings_df.index:\n",
        "            p0 = abf_recordings_df.at[rec,p[0]]\n",
        "            p1 = abf_recordings_df.at[rec,p[1]]\n",
        "            if np.isnan(p0) and np.isnan(p1):\n",
        "                abf_recordings_df.at[rec,col_name] = np.nan\n",
        "            else:\n",
        "                if np.isnan(p0):\n",
        "                    abf_recordings_df.at[rec,col_name] = p1\n",
        "                elif np.isnan(p1):\n",
        "                    abf_recordings_df.at[rec,col_name] = p0\n",
        "                else:\n",
        "                    print('unspecified types:', p0,p1)\n",
        "                    abf_recordings_df.at[rec,col_name] = np.nan\n",
        "    return abf_recordings_df\n"
      ],
      "metadata": {
        "id": "XdC03YaXX2eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#################################### Summarize Data Cell by Cell ####################################\n",
        "def parse_file_name(abf_recordings_df):\n",
        "    'Takes the abf data frame and sorts the files in to groups based on cell ids in the abf file name.'\n",
        "    'Parses the file name extract the cell meta data and stores in a new summary dataframe'\n",
        "    'The new dataframe is populated with measured data from each abf. repeated measures are combined into'\n",
        "    'a list'\n",
        "\n",
        "    cell_list = []\n",
        "    abf_recordings_df.sort_values('file_name',inplace=True)\n",
        "    for f in abf_recordings_df.index:\n",
        "        cell_id = f[f.rfind('/')+1:] # drop directory info\n",
        "        cell_id = cell_id[:cell_id.find('.abf')] # drop file extension\n",
        "        cell_id = cell_id[:cell_id.rfind('_')] # drop rec number\n",
        "        abf_recordings_df.at[f,'cell_id'] = cell_id # write Id\n",
        "        cell_list.append(cell_id) # keep a list\n",
        "    # print(cell_list)\n",
        " \n",
        "    # populate new cell based dataframe\n",
        "    cell_list = list(set(cell_list))\n",
        "    cell_df = pd.DataFrame( {'cell_id': cell_list}).set_index('cell_id')\n",
        "\n",
        "\n",
        "    verbose = False\n",
        "    # LABELING\n",
        "    for c in cell_df.index:\n",
        "        dashes = [i for i in range(len(str(c))) if '_' in str(c)[i] ]\n",
        "        if verbose: _ = [print('   '+ c[:d]) for d in dashes   ]\n",
        "        date = c[:dashes[0]].lower()\n",
        "        virus = c[dashes[0]+1:dashes[1]].upper()\n",
        "        genotype = c[dashes[1]+1:dashes[2]].upper()\n",
        "        sex = c[dashes[2]+1:dashes[3]]\n",
        "        age =c[dashes[3]+1:dashes[4]].upper()\n",
        "        slice_num = c[dashes[4]+1:dashes[5]].upper()\n",
        "        cell_num = c[dashes[5]+1:].upper()\n",
        "        cell_type = c[dashes[6]+1:].upper()\n",
        "\n",
        "        age = int(age[1:])\n",
        "\n",
        "        # keeps\n",
        "        cell_df.at[c,'Date'] = date\n",
        "        cell_df.at[c,'virus'] = virus\n",
        "        cell_df.at[c,'geno'] = genotype\n",
        "        cell_df.at[c,'age (days)'] = age\n",
        "        cell_df.at[c,'sex'] = sex\n",
        "        cell_df.at[c,'slice'] = slice_num\n",
        "        cell_df.at[c,'cell_type'] = cell_type\n",
        "        if all([age>=(7*30), age<((9+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '7_to_9'\n",
        "        elif all([age>=(17*30), age<((19+1)*30)]):\n",
        "            cell_df.at[c,'Age_bin'] = '17_to_19'\n",
        "        elif all([age>=(2*30), age<((5+1)*30)]): \n",
        "            cell_df.at[c,'Age_bin'] = '2_to_5'\n",
        "        else:\n",
        "            cell_df.at[c,'Age_bin'] = 'other'\n",
        "\n",
        "    date_to_animal = {}\n",
        "    set_list = list(set(cell_df['Date']))\n",
        "    for i in range(len(set_list)):\n",
        "        d = set_list[i]\n",
        "        date_to_animal[d] = i\n",
        "    \n",
        "    for c in cell_df.index:\n",
        "        d = cell_df.at[c,'Date']\n",
        "        cell_df.at[c,'Animal'] = date_to_animal[d]\n",
        "\n",
        "    #### Actual Values\n",
        "    cell_df['rec_list'] = None\n",
        "    for c in cell_df.index:\n",
        "        is_cell = abf_recordings_df['cell_id'] == c\n",
        "        cell_recs = list(abf_recordings_df.index[is_cell])\n",
        "        cell_df.at[c,'rec_list'] = cell_recs\n",
        "\n",
        "    data_columns = abf_recordings_df.columns\n",
        "    data_columns = [c for c in data_columns if 'cell_id' not in c]\n",
        "    for dc in data_columns:\n",
        "        cell_df[dc] = None\n",
        "        for c in cell_df.index:\n",
        "            is_cell = abf_recordings_df['cell_id'] == c\n",
        "            cell_data = list(abf_recordings_df.loc[is_cell,dc])\n",
        "            if not(type(cell_data[0]) == str):\n",
        "                # remove recs that report 'none'\n",
        "                cd = cell_data[0]\n",
        "                cell_data = [cd for cd in cell_data if not( str(type(cd)) == str(type(None)) )]\n",
        "                # remove recs that report 'nan' but check if its a float first because np.isnan is a little bitch.\n",
        "                float_data = []\n",
        "                for cd in cell_data:\n",
        "                    if str(type(cd)) == \"<class 'float'>\":\n",
        "                        if np.isnan(cd):\n",
        "                            'skip'\n",
        "                        else:\n",
        "                            float_data.append(cd)\n",
        "                    else:\n",
        "                        float_data.append(cd)\n",
        "\n",
        "                cell_data = float_data\n",
        "                # print('new',cell_data)\n",
        "                \n",
        "            cell_df.at[c,dc] = np.array(cell_data)\n",
        "\n",
        "    for c in cell_df.index:\n",
        "        prots = cell_df.at[c,'protocol']\n",
        "        cell_df.at[c,'protocol'] = set(prots)\n",
        "\n",
        "    print(len(cell_df),' cells from',len(set(cell_df['Date'])),'animals')\n",
        "\n",
        "    return cell_df, abf_recordings_df\n"
      ],
      "metadata": {
        "id": "WafNAcyEGhk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#################### Pull Data From Summary ##############################\n",
        "\n",
        "def get_summary_data(cell_summary_df, data_keys, conditional_dict,filter_empty=False,single_val=False):\n",
        "    'WIP. Takes in a summary dataframe, a list of data keys to look up, and'\n",
        "    'as dict of cell type conditions eg: {\"cell_type\"; \"CA3\"}. Returns cell data.'\n",
        "    'Options include collapsing lists of repeat measures into a single value, or removing'\n",
        "    'cells without values for all data keys (any missing value disqualifies the cell)'\n",
        "\n",
        "\n",
        "    condition_list = [] \n",
        "    for (k,v) in conditional_dict.items():\n",
        "        cond_met =cell_summary_df[k] == v\n",
        "        condition_list.append( cond_met )\n",
        "\n",
        "    condition_list = np.stack(condition_list).T\n",
        "    all_met = [all(r) for r in condition_list]\n",
        "    all_met_ind = cell_summary_df.index[all_met]\n",
        "    cell_vals = {}\n",
        "    for dk in data_keys:\n",
        "        cell_vals[dk] = cell_summary_df.loc[all_met_ind,dk].values\n",
        "   \n",
        "    not_empty_list = []\n",
        "    for dk in data_keys:\n",
        "        # print(cell_vals[dk])\n",
        "        # [print(v) for v in cell_vals[dk]]\n",
        "        # cell_vals[dk] = [ [] if  else v for v in cell_vals[dk] ]\n",
        "        not_empty = [ len(v)>0 for v in cell_vals[dk] ]\n",
        "        not_empty_list.append( not_empty )\n",
        "\n",
        "    not_empty_list = np.stack(not_empty_list).T\n",
        "    all_not_empty = [all(r) for r in not_empty_list]\n",
        "    if filter_empty:\n",
        "        for dk in data_keys:\n",
        "            cell_vals[dk] = cell_vals[dk][all_not_empty]\n",
        "\n",
        "    if single_val:\n",
        "        for dk in data_keys:\n",
        "            # cell_vals[dk] = np.array([v[0] if len(v)>0 else v for v in cell_vals[dk]]) # first value\n",
        "            # cell_vals[dk] = np.array([np.mean(v) for v in cell_vals[dk]]) # mean value\n",
        "            cell_vals[dk] = np.array([np.median(v) for v in cell_vals[dk]]) # median value\n",
        "\n",
        "    return cell_vals, all_met, all_not_empty\n",
        "\n"
      ],
      "metadata": {
        "id": "Oj9ESi3oGYo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### ANALYZE IV CURVE #######################\n",
        "\n",
        "\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "def IV_analyisis(abf_recordings_df,protocol_aliases, post_stim_times=[(0.0165,.03),(0.100,0.120)], stats_to_split= [(0, 'I_peak'),  (0, 'V_stim'), (1, 'I_mean')],to_plot=False):\n",
        "    'Calulates results from an IV protocol. The peak and mean are measured'\n",
        "    'from windows defined in a list of tuples [(start,stop),...]. All values'\n",
        "    'are written to the main dataframe under \"IV_stats\". This is a list of dicts'\n",
        "    'each dict corresponds to one anaysis window reporting the time range, peak etc.'\n",
        "    'Optional variable stats_to_split is used to pull specific measures from IV_stats'\n",
        "    'and write them to their own column. Stats_to_split is a list of tuples, indicating'\n",
        "    'which window and witch measure to write to the dataframe. [(window index,measure key)...]'\n",
        "\n",
        "\n",
        "    correct_protocol = [ p in protocol_aliases for p in abf_recordings_df['protocol']]\n",
        "    abf_recordings_df['IV_stats'] = None\n",
        "    abf_recordings_df['IV_stats'] = abf_recordings_df['IV_stats'].astype(object)\n",
        "\n",
        "    stats_to_split = ( (0, 'I_peak'),  (0, 'V_stim'), ( (1, 'I_mean')))\n",
        "    for t in stats_to_split:\n",
        "        name = t[1]\n",
        "        abf_recordings_df['IV_'+name] = None\n",
        "        abf_recordings_df['IV_'+name] = abf_recordings_df['IV_'+name].astype(object)\n",
        "\n",
        "    for file_name in tqdm( abf_recordings_df.index[correct_protocol]):\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        passing_sweeps=abf_recordings_df.at[file_name,'passing_sweeps']\n",
        "\n",
        "        pass_rate, passing_sweeps, QC_check_df, QC_val_df = Vclamp_QC(file_name,to_plot=False,verbose=False,Vhold_range = 100)\n",
        "        # display(QC_val_df)\n",
        "        if to_plot: plot_sweeps_and_command(abf)\n",
        "        # passing_sweeps = abf.sweepList # OVERRIDE\n",
        "        print(passing_sweeps)\n",
        "        abf = pyabf.ABF( file_name )\n",
        "        if to_plot: print(file_name)\n",
        "        IV_stats =analyze_IV(abf, passing_sweeps, post_stim_times,to_plot =to_plot)\n",
        "        # [print(r) for r in IV_stats]\n",
        "        \n",
        "        abf_recordings_df.at[file_name,'IV_stats'] = IV_stats\n",
        "\n",
        "        #### \n",
        "        if len(IV_stats) > 0:\n",
        "            for t in stats_to_split:\n",
        "                name = t[1]\n",
        "                range = t[0]\n",
        "                values = IV_stats[range][name]\n",
        "                values = np.array(values).tolist()\n",
        "                abf_recordings_df.at[file_name,'IV_'+name] = values\n",
        "\n",
        "    return abf_recordings_df\n",
        "\n",
        "\n",
        "\n",
        "def get_IV_measures(abf,passing_sweeps,post_stim_times,to_plot=False,spike_thresh=100000):\n",
        "    'Takes the abf data and calculates the mean and peak for each window.'\n",
        "    'Optionaly sweeps with APs can be filtered out using spike_threshold.'\n",
        "    'Only sweeps passing the QC filter are used. Returns a list of dicts with'\n",
        "    'each dict corresponding to one anaysis window reporting the time range, peak etc.'\n",
        "\n",
        "    IV_stats = []\n",
        "    theta, offset, correct_ch1 = predict_telegraph(abf)\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    if len(passing_sweeps)==0:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(len(post_stim_times),figsize = [12,4] )\n",
        "        if len(post_stim_times)==1: axs = [axs]\n",
        "    \n",
        "    for t in range(len(post_stim_times)):\n",
        "        window_stats = {}\n",
        "        start = post_stim_times[t][0]\n",
        "        stop = post_stim_times[t][1]\n",
        "        delta_t = stop-start\n",
        "        flank_neg = start - (delta_t)*0.5\n",
        "        flank_pos = stop + (delta_t)*0.5\n",
        "        full_plot_range_ind =   np.logical_and(  abf.sweepX>flank_neg, abf.sweepX<flank_pos )\n",
        "        spikeless_sweeps = []\n",
        "        for s in passing_sweeps:\n",
        "            if to_plot: s_shift_t = delta_t*0.03*s\n",
        "            if to_plot: s_shift_I = 100*s\n",
        "            abf.setSweep(s)\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = movmean( abf.sweepY[analysis_range], 4)\n",
        "            dIdt = movmean(np.diff(I_abrg, prepend=I_abrg[0]),1)\n",
        "\n",
        "            if np.max(abs(dIdt))>spike_thresh:\n",
        "                'indent holder'\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'r')\n",
        "            else:\n",
        "                # if to_plot:axs[t].plot( time_abrg+s_shift_t, dIdt+s_shift_I,'k')\n",
        "                spikeless_sweeps.append(s)\n",
        "\n",
        "\n",
        "        I_peak_l = []\n",
        "        time_peak_l = []\n",
        "        I_mean_l = []\n",
        "        V_stim_l = []\n",
        "        \n",
        "        for s in  spikeless_sweeps :\n",
        "            abf.setSweep(s)\n",
        "            s_shift_t = delta_t*0.03*s * 0\n",
        "            s_shift_I = 100*s * 0\n",
        "\n",
        "            time_abrg = abf.sweepX[full_plot_range_ind]\n",
        "            I_abrg = abf.sweepY[full_plot_range_ind]\n",
        "            analysis_range = np.logical_and(  abf.sweepX>start, abf.sweepX<stop )\n",
        "            time_analysis = abf.sweepX[analysis_range]\n",
        "            I_analysis = abf.sweepY[analysis_range]\n",
        "            \n",
        "\n",
        "            # Peaks\n",
        "            if to_plot: axs[t].plot( time_abrg+s_shift_t, I_abrg+s_shift_I,'k')\n",
        "            \n",
        "            I_mean = np.median( I_analysis )\n",
        "            if to_plot: axs[t].plot( time_analysis+s_shift_t, (I_mean*np.ones_like(time_analysis))+s_shift_I,'r' )            \n",
        "            \n",
        "            local_I_mean = movmean(I_analysis,100)\n",
        "            peak_ind = np.where(abs(I_analysis-I_mean) == np.max(abs(I_analysis-I_mean)))[0]\n",
        "            # print(peak_ind)\n",
        "            if len(peak_ind)>1: peak_ind=[np.min(peak_ind)]\n",
        "                # print('drop')\n",
        "                # print(peak_ind)\n",
        "\n",
        "            I_peak =I_analysis[peak_ind]\n",
        "            t_peak = time_analysis[peak_ind]\n",
        "            if to_plot: axs[t].scatter( t_peak+s_shift_t, I_peak+s_shift_I, color='m' )\n",
        "\n",
        "            abf.setSweep(sweepNumber=s, channel=1)\n",
        "            command_trace = abf.sweepY\n",
        "\n",
        "\n",
        "            I_peak_l.append(I_peak)\n",
        "            time_peak_l.append(t_peak)\n",
        "            I_mean_l.append(I_mean)\n",
        "            # raw_v = np.median( abf.sweepC[analysis_range])\n",
        "            # V_stim_l.append( 10*np.round( (raw_v +offset )/10))\n",
        "\n",
        "            command_v = np.median( command_trace[analysis_range])\n",
        "            V_stim_l.append(command_v)\n",
        "    \n",
        "\n",
        "        I_peak_l = np.concatenate(I_peak_l)\n",
        "        time_peak_l = np.concatenate(time_peak_l)\n",
        "\n",
        "\n",
        "        # print(I_peak_l)\n",
        "        # print(time_peak_l)\n",
        "        # print(I_mean_l)\n",
        "        # print(V_stim_l)\n",
        "\n",
        "\n",
        "        window_stats['range'] = (start,stop)\n",
        "        window_stats['I_peak'] = I_peak_l\n",
        "        window_stats['I_peak_time'] = time_peak_l\n",
        "        window_stats['I_mean'] = I_mean_l\n",
        "        window_stats['V_stim'] = np.stack(V_stim_l).flatten()\n",
        "        IV_stats.append(window_stats)\n",
        "\n",
        "    return IV_stats\n",
        "    \n",
        "\n",
        "def analyze_IV(abf, passing_sweeps, post_stim_times, to_plot=False):\n",
        "    'This is largely a wrapper for get_IV_measures(). The main role'\n",
        "    'of this function is to plot the summary data, ie the traditional IV plot'\n",
        "\n",
        "    IV_stats = get_IV_measures(abf, passing_sweeps,post_stim_times,to_plot=to_plot,spike_thresh=1e6)\n",
        "    any_sweeps = any(np.greater([len(w['V_stim']) for w in IV_stats],0))\n",
        "    if not any_sweeps:\n",
        "        return IV_stats\n",
        "\n",
        "    if to_plot:\n",
        "        fig, axs = plt.subplots(1,2,figsize = (9,4))\n",
        "        early_peak = IV_stats[0]['I_peak']\n",
        "        V_stim = IV_stats[0]['V_stim']\n",
        "        axs[0].plot(V_stim,early_peak,color='m',marker='o',label='Peak Current')\n",
        "\n",
        "\n",
        "        # ax.scatter(V_stim,early_peak,color='k')\n",
        "        late_mean = IV_stats[1]['I_mean']\n",
        "        V_stim = IV_stats[1]['V_stim']\n",
        "        axs[0].plot(V_stim,late_mean,color='r',marker='o',label='Steady State Current')\n",
        "\n",
        "        from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
        "        axs[0].xaxis.set_major_locator(MultipleLocator(20))\n",
        "        axs[0].legend(loc=\"upper left\",frameon=True,framealpha=1)\n",
        "        axs[0].set_ylabel('Current (pA)')\n",
        "        axs[0].set_xlabel('Membrane Potential (mV)')\n",
        "\n",
        "        \n",
        "        # ax.grid(True)\n",
        "        axs[0].spines['top'].set_visible(False)\n",
        "        axs[0].spines['right'].set_visible(False)\n",
        "        axs[0].spines['bottom'].set_visible(False)\n",
        "        axs[0].spines['left'].set_visible(False)\n",
        "        yL = axs[0].get_ylim()\n",
        "        xL = axs[0].get_xlim()\n",
        "        axs[0].plot(xL,[0,0],':k')\n",
        "        axs[0].plot([-70,-70],yL,':k')\n",
        "\n",
        "\n",
        "        for s in passing_sweeps:\n",
        "            abf.setSweep(s)\n",
        "            axs[1].plot(abf.sweepX,abf.sweepC,'k')\n",
        "        \n",
        "        yL = axs[1].get_ylim()\n",
        "        axs[1].axvspan( IV_stats[0]['range'][0],IV_stats[0]['range'][1],yL[0],yL[1], alpha=0.2 ,color='m')\n",
        "        axs[1].axvspan( IV_stats[1]['range'][0],IV_stats[1]['range'][1],yL[0],yL[1], alpha=0.2 ,color='r')\n",
        "        plt.show()\n",
        "    return IV_stats\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DJTX1AU04ypU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##################### STREAMLINE SUMMARY ###########################\n",
        "def stream_line_cell_summary(cell_summary_df_messy):\n",
        "    'Takes in the cell-summary_df and stream lines it into single'\n",
        "    'measures per cell that can be easily collated into a readable csv'\n",
        "    'that is broadly shareable between analysis platforms.'\n",
        "    cell_summary_df =pd.DataFrame(cell_summary_df_messy.index ).set_index('cell_id')\n",
        "\n",
        "    # print(cell_summary_df_messy.columns)\n",
        "\n",
        "    'Cell Details'\n",
        "    cols = ['Date', 'geno', 'age (days)', 'sex', 'slice', 'cell_type', 'Age_bin']\n",
        "    # Copy relevant info\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "\n",
        "\n",
        "    'Fast Pulse Capcitance VC'\n",
        "    cols = ['Ra_10.0','Rm_10.0','Cmq_10.0','Cm_pc_10.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "    \n",
        "    'Slow Pulse Capcitance VC'\n",
        "    cols = ['Ra_160.0','Rm_160.0','Cmq_160.0','Cm_pc_160.0']\n",
        "    # Test Pulse No QC here\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df.at[id,c]\n",
        "            consolidated = np.mean(multi_values)\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "\n",
        "    'Spike Latency - IC'\n",
        "    cols = ['Spike_Latency_(ms)', 'V_hold_(Latency)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge\n",
        "    # Consolidate with averaging\n",
        "    for id in cell_summary_df.index:\n",
        "        multi_values_SL = cell_summary_df_messy.at[id,'Spike_Latency_(ms)']\n",
        "        multi_values_VH = cell_summary_df_messy.at[id,'V_hold_(Latency)']\n",
        "        condition  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "        good_vals = [multi_values_SL[i] for i in range(len(multi_values_SL)) if condition[i]]\n",
        "        consolidated = np.mean(  good_vals  )\n",
        "        cell_summary_df.at[id,'Spike_Latency_(ms)'] = consolidated\n",
        "\n",
        "\n",
        "    'Rheobase - IC'\n",
        "    cols = ['Rheobase_(pA)', 'AP_Threshold(mV)', 'Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)']\n",
        "    # QC: V_hold should be -70+/-3 to get consistant charge. I_hold_(pA) should be ~<100 for good clamping\n",
        "    # Consolidate with mean\n",
        "    for c in cols[:4]:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            multi_values_VH = cell_summary_df_messy.at[id,'Vhold_Rheo_(mV)']\n",
        "            multi_values_IH = cell_summary_df_messy.at[id,'Rheo Ihold_(pA)']\n",
        "            condition_V  = np.logical_and(multi_values_VH>-80,multi_values_VH<-60)\n",
        "            condition_I  = abs(multi_values_IH)<250\n",
        "            condition  = np.logical_and(condition_V,condition_I)\n",
        "            good_vals = [multi_values[i] for i in range(len(multi_values)) if condition[i]]\n",
        "            consolidated = np.mean(  good_vals  )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Input Resistance - IC'\n",
        "    cols = ['Rinput_(MO)', 'Cmf_IC_(pF)']\n",
        "    # QC: None\n",
        "    # Consolidate with averaging\n",
        "    for c in cols:\n",
        "        for id in cell_summary_df.index:\n",
        "            multi_values = cell_summary_df_messy.at[id,c]\n",
        "            consolidated = np.mean(  multi_values )\n",
        "            cell_summary_df.at[id,c] = consolidated\n",
        "\n",
        "\n",
        "    'Fireing Rate Gain - IC'\n",
        "    cols = ['Firing_Gain_(Hz/pA)','R2 (Firing_Gain_R2)', 'Firing_Duration_%','Gain_Stims_pA','Gain_NumSpikes']\n",
        "    # QC: R2 (r squared) should be decent so that (>0.8) we fit a reasonable section of curve with a line\n",
        "    # Consolidate by selecting best Range of Response\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "    for id in cell_summary_df.index:\n",
        "\n",
        "        try:\n",
        "            multi_values_Spikes = cell_summary_df_messy.at[id,'Gain_NumSpikes']\n",
        "            \n",
        "            range_list =[]\n",
        "            for trial in range(len(multi_values_Spikes)):\n",
        "                range_list.append(np.max( multi_values_Spikes[trial]) - np.min( multi_values_Spikes[trial]))\n",
        "            if len(multi_values_Spikes) > 0  :             \n",
        "                chosen_one = np.where(range_list == np.max(range_list))[0]\n",
        "                if len(chosen_one)>0: chosen_one = chosen_one[0]\n",
        "                for c in cols: \n",
        "                    if len(cell_summary_df_messy.at[id,c]) == 0:\n",
        "                        cell_summary_df.at[id,c] = np.nan\n",
        "                    else:\n",
        "                        consolidated = cell_summary_df_messy.at[id,c][chosen_one] # fails Here\n",
        "                        cell_summary_df.at[id,c] = consolidated.tolist()\n",
        "                    if c=='Firing_Duration_%':\n",
        "                        cell_summary_df.at[id,c] = np.nanmedian(cell_summary_df_messy.at[id,c])\n",
        "            else:\n",
        "                for c in cols:  \n",
        "                    cell_summary_df.at[id,c] = np.nan\n",
        "        except: \n",
        "            print('error on ', id)\n",
        "            print( id in cell_summary_df.index )\n",
        "            display(cell_summary_df.loc[id])\n",
        "            print(multi_values_Spikes)\n",
        "            print(range_list)\n",
        "            print('c',c)\n",
        "\n",
        "    'IV ~I_Na , ~I_K'\n",
        "    cols = ['IV_V_stim', 'IV_I_peak', 'IV_I_mean']\n",
        "    # QC:  Look For normal Na 'swoop' and 'opposing Kv currents\n",
        "    # By eyeball, linear currents should not be extreme.\n",
        "    # Consolidate with averaging\n",
        "    cell_summary_df[cols] = cell_summary_df_messy[cols]\n",
        "\n",
        "    for id in cell_summary_df.index:\n",
        "        # print(id)\n",
        "        stim_values = cell_summary_df_messy['IV_V_stim'][id]\n",
        "        stim_values = [b for a in stim_values for b in a]\n",
        "        # print('stim_values', stim_values)\n",
        "        if len(stim_values)>0:\n",
        "            stim_set = list(set(stim_values))\n",
        "            stim_set.sort()\n",
        "            cell_summary_df.at[id,'IV_V_stim']=stim_set\n",
        "            for c in cols[1:]:\n",
        "                resp_vals = cell_summary_df_messy[c][id]\n",
        "                flattened_vals = [b for a in resp_vals for b in a]\n",
        "                # print('flattened_responses',flattened_vals)\n",
        "                response_list = []\n",
        "                for v in stim_set:\n",
        "                    # print(\"stim_values\",len(stim_values),v)\n",
        "                    # print('logical', stim_values == v)\n",
        "                    index_to_avg = np.where( np.equal(stim_values, v))\n",
        "                    # print('index_to_avg',index_to_avg)\n",
        "                    vals_to_avg = flattened_vals[index_to_avg[0][0]]\n",
        "                    # print('vals_to_avg',vals_to_avg)\n",
        "                    response_list.append( np.mean(vals_to_avg))\n",
        "                response_array = np.stack(response_list).tolist()\n",
        "                cell_summary_df.at[id,c]=response_array\n",
        "\n",
        "    # for c in cols:\n",
        "    #     cell_summary_df[c] = None\n",
        "    #     cell_summary_df[c] = cell_summary_df[c].astype(object)\n",
        "    #     for id in cell_summary_df.index:\n",
        "    #         print(cell_summary_df_messy.loc[id, 'IV_V_stim'])\n",
        "    #         multi_values = cell_summary_df_messy.at[id,c]\n",
        "    #         if len(multi_values)==0:\n",
        "    #             continue\n",
        "    #         multi_values = np.stack(multi_values)\n",
        "    #         consolidated = np.mean(multi_values,axis=0)\n",
        "    #         cell_summary_df.at[id,c] = consolidated\n",
        "    return cell_summary_df\n",
        "\n"
      ],
      "metadata": {
        "id": "UG6nSYbMMF0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_store_data(abf_recordings_df):\n",
        "\n",
        "    unused = ['Ra_5.0','Rm_5.0','Cmq_5.0','Ra_Combo','Rm_Combo','Cmq_Combo']\n",
        "\n",
        "    abrg_col = ['protocol','abf_timestamp','Ra_10.0','Rm_10.0','Cmq_10.0', 'Ra_160.0','Rm_160.0','Cmq_160.0', 'Cm_pc_10.0',  'Cm_pc_160.0',\n",
        "             'Gain_Stims_pA','Gain_NumSpikes', 'Firing_Gain_(Hz/pA)', 'R2 (Firing_Gain_R2)', 'Firing_Duration_%',\n",
        "            'Rheobase_(pA)','AP_Threshold(mV)', \n",
        "            'Rinput_(MO)','Spike_Latency_(ms)', \n",
        "            'IV_I_peak', 'IV_V_stim', 'IV_I_mean',\n",
        "            'Cmf_IC_(pF)','Vhold_Rheo_(mV)', 'Rheo Ihold_(pA)','V_hold_(Latency)','passing_sweeps',]\n",
        "\n",
        "    abrg_abf_recs_df = abf_recordings_df[abrg_col].copy()\n",
        "    cell_summary_df, abf_recordings_df_tagged = parse_file_name(abrg_abf_recs_df)\n",
        "    cell_summary_df.sort_index(inplace=True)\n",
        "    \n",
        "    #################### Store Data ##################\n",
        "    abrg_abf_recs_df.to_hdf('abrg_abf_recs_df.hdf','abrg_abf_recs_df')\n",
        "    abrg_abf_recs_df.to_csv('abrg_abf_recs_df.csv')\n",
        "    abf_recordings_df_tagged.to_csv('abf_recordings_df_tagged.csv')\n",
        "    cell_summary_df.to_csv('cell_summary_df.csv')\n",
        "\n",
        "\n",
        "    from google.colab import files\n",
        "    files.download('abrg_abf_recs_df.csv')\n",
        "    files.download('abf_recordings_df_tagged.csv')\n",
        "    files.download('cell_summary_df.csv') \n",
        "\n",
        "    cell_summary_df_clean = stream_line_cell_summary(cell_summary_df)\n",
        "    cell_summary_df_clean.to_csv('cell_summary_df_clean.csv')\n",
        "    files.download('cell_summary_df_clean.csv')\n",
        "\n",
        "    return cell_summary_df_clean, abrg_abf_recs_df\n",
        "\n"
      ],
      "metadata": {
        "id": "uDhjbAyGhbf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############## The PipeLine Cell ###############################\n",
        "####### Depenencies ###########\n",
        "!pip install --upgrade pyabf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyabf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from scipy import stats\n",
        "import os\n",
        "from scipy.signal import butter,filtfilt\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "clear_output(wait=False)\n",
        "import sys\n",
        "import warnings\n",
        "from google.colab import files\n",
        "warnings.filterwarnings('ignore')\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "############ Get Files & Populate Data Frame ############\n",
        "\n",
        "new_filename = str(datetime.now())[:10]+\"_hipp_data.zip\"\n",
        "new_filename = \"_hipp_data.zip\"\n",
        "file_loc = get_drobox_folder(link, new_filename)\n",
        "clear_output(wait=False)\n",
        "cell_id_order = ['Rec_date','Virus','GenoType','Sex','Age','Slice_Num','Cell_num','Cell_Type']\n",
        "abf_recordings_df,protocol_set = catalogue_recs(file_loc,cell_id_order)\n",
        "print(len(abf_recordings_df), 'files')\n",
        "print('Protocols:')\n",
        "_=[print(\"   \",p) for p in protocol_set]\n",
        "# print('ABFS:')\n",
        "# _=[print(\"   \",p) for p in abf_recordings_df.index]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_1cfqD5DbUDc",
        "outputId": "06123e0b-5d40-48a9-f7b9-b12f8747926d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "256 files\n",
            "Protocols:\n",
            "    IC - R input\n",
            "    VC - MemTest-10ms-160ms\n",
            "    IC - Gain - D50pA\n",
            "    VC - Multi IV - 150ms\n",
            "    VC - 3min GapFree\n",
            "    IC - Latentcy 800pA-1s\n",
            "    IC - Sag - D50pA\n",
            "    IC - Rheobase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "note_book = abf_recordings_df[abf_recordings_df.columns]\n",
        "note_book.set_index('Recording_name',inplace=True)\n",
        "note_book.drop(labels=['channelList'],axis=1,inplace=True)\n",
        "note_book.to_csv('note_book_page.csv')\n",
        "files.download('note_book_page.csv')\n",
        "display(note_book)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "JZgHgpbG-RgB",
        "outputId": "6e57c9f2-3622-46e4-c957-8cc405a74375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2f27c1fe-a585-4964-a873-330ad249e06f\", \"note_book_page.csv\", 50250)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                                                            cell_id  \\\n",
              "Recording_name                                                                                        \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS   \n",
              "...                                                                                             ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG   \n",
              "\n",
              "                                                      Rec_date   Virus  \\\n",
              "Recording_name                                                           \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022x08x12  RNF182   \n",
              "...                                                        ...     ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022x08x17  RNF182   \n",
              "\n",
              "                                                   GenoType Sex   Age  \\\n",
              "Recording_name                                                          \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...     E4KI   F  P251   \n",
              "...                                                     ...  ..   ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...     E4KI   F  P256   \n",
              "\n",
              "                                                   Slice_Num Cell_num  \\\n",
              "Recording_name                                                          \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...      s001     c001   \n",
              "...                                                      ...      ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...      s002     c010   \n",
              "\n",
              "                                                   Cell_Type  \\\n",
              "Recording_name                                                 \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...   CA3xPOS   \n",
              "...                                                      ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   CA3xNEG   \n",
              "\n",
              "                                                                   protocol  \\\n",
              "Recording_name                                                                \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...        VC - 3min GapFree   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  VC - MemTest-10ms-160ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...    VC - Multi IV - 150ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...    VC - Multi IV - 150ms   \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...        IC - Gain - D50pA   \n",
              "...                                                                     ...   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...   IC - Latentcy 800pA-1s   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...             IC - R input   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...    VC - Multi IV - 150ms   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  VC - MemTest-10ms-160ms   \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  VC - MemTest-10ms-160ms   \n",
              "\n",
              "                                                              abf_timestamp  \n",
              "Recording_name                                                               \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:00.750  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:16.222  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:36.142  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:06:42.833  \n",
              "2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS...  2022-08-12T12:07:28.480  \n",
              "...                                                                     ...  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:32:59.098  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:33:42.410  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:34:50.985  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:35:28.268  \n",
              "2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG...  2022-08-17T17:35:43.981  \n",
              "\n",
              "[256 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7df3575f-a97c-479e-88fa-de0006ae12be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cell_id</th>\n",
              "      <th>Rec_date</th>\n",
              "      <th>Virus</th>\n",
              "      <th>GenoType</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>Slice_Num</th>\n",
              "      <th>Cell_num</th>\n",
              "      <th>Cell_Type</th>\n",
              "      <th>protocol</th>\n",
              "      <th>abf_timestamp</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Recording_name</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0000.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - 3min GapFree</td>\n",
              "      <td>2022-08-12T12:06:00.750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0001.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-12T12:06:16.222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0002.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-12T12:06:36.142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0003.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-12T12:06:42.833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS_0004.abf</th>\n",
              "      <td>2022x08x12_RNF182_E4KI_F_P251_s001_c001_CA3xPOS</td>\n",
              "      <td>2022x08x12</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P251</td>\n",
              "      <td>s001</td>\n",
              "      <td>c001</td>\n",
              "      <td>CA3xPOS</td>\n",
              "      <td>IC - Gain - D50pA</td>\n",
              "      <td>2022-08-12T12:07:28.480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0006.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>IC - Latentcy 800pA-1s</td>\n",
              "      <td>2022-08-17T17:32:59.098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0007.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>IC - R input</td>\n",
              "      <td>2022-08-17T17:33:42.410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0008.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - Multi IV - 150ms</td>\n",
              "      <td>2022-08-17T17:34:50.985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0009.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-17T17:35:28.268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG_0010.abf</th>\n",
              "      <td>2022x08x17_RNF182_E4KI_F_P256_s002_c010_CA3xNEG</td>\n",
              "      <td>2022x08x17</td>\n",
              "      <td>RNF182</td>\n",
              "      <td>E4KI</td>\n",
              "      <td>F</td>\n",
              "      <td>P256</td>\n",
              "      <td>s002</td>\n",
              "      <td>c010</td>\n",
              "      <td>CA3xNEG</td>\n",
              "      <td>VC - MemTest-10ms-160ms</td>\n",
              "      <td>2022-08-17T17:35:43.981</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>256 rows  11 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7df3575f-a97c-479e-88fa-de0006ae12be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7df3575f-a97c-479e-88fa-de0006ae12be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7df3575f-a97c-479e-88fa-de0006ae12be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### Run QC tests ###############\n",
        "print('running QC...')\n",
        "VC_prot = ['VC - MemTest-10ms-160ms',\n",
        "           'VC - Multi IV - 150ms',\n",
        "           'VC - 3min GapFree',\n",
        "           'VC - Spontaneous-3min-(MT)']\n",
        "IC_prot = ['IC - Gain - D20pA',\n",
        "           'IC - Gain - D50pA',\n",
        "           'IC - Rheobase',\n",
        "           'IC - R input',\n",
        "           'IC - Latentcy 800pA-1s']\n",
        "MT_prot = ['VC - MemTest-10ms-160ms']\n",
        "\n",
        "\n",
        "abf_recordings_df = QC_full_dataset(abf_recordings_df,to_plot=False,verbose=False,VC_prot=VC_prot,IC_prot=IC_prot,MT_prot=MT_prot)\n"
      ],
      "metadata": {
        "id": "J2bawVQmVmB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5507abf-fd72-4ab9-bdae-33b66ebb96fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running QC...\n",
            "Voltage Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/103 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  8%|         | 8/103 [00:00<00:06, 14.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|         | 10/103 [00:00<00:07, 13.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|        | 17/103 [00:01<00:05, 14.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|        | 21/103 [00:01<00:05, 14.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 27%|       | 28/103 [00:02<00:04, 15.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|       | 32/103 [00:02<00:04, 14.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|      | 36/103 [00:02<00:04, 14.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|      | 38/103 [00:02<00:04, 15.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|      | 42/103 [00:03<00:05, 10.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|     | 44/103 [00:03<00:05, 11.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|     | 48/103 [00:03<00:05,  9.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|     | 50/103 [00:04<00:05, 10.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 54%|    | 56/103 [00:04<00:05,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|    | 62/103 [00:05<00:04,  9.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|   | 65/103 [00:05<00:04,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 68%|   | 70/103 [00:06<00:03,  8.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|   | 73/103 [00:06<00:03,  8.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|  | 77/103 [00:07<00:03,  8.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|  | 78/103 [00:07<00:03,  7.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%| | 85/103 [00:08<00:02,  8.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n",
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%| | 89/103 [00:08<00:02,  6.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|| 95/103 [00:09<00:01,  7.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|| 99/103 [00:10<00:00,  8.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrong clamp mode for protocol. Voltage protocol used during current clamp!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 103/103 [00:10<00:00,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Clamp Protocols\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 137/137 [00:42<00:00,  3.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## Multi Mem Test ########################\n",
        "print('Passive Membrane params...')\n",
        "protocol_aliases = ['VC - MemTest-10ms-160ms']\n",
        "abf_recordings_df, correct_protocol = Icapacitance_analysis(abf_recordings_df, protocol_aliases,to_plot=False,verbose=False)\n",
        "\n",
        "################### Rheobase Analyisis ###########################\n",
        "print('Rheobase Analyisis...')\n",
        "Rheo_aliases = ['IC - Rheobase']\n",
        "abf_recordings_df = Rheo_curve_analysis(abf_recordings_df, Rheo_aliases,to_plot = False,single_spike=False)\n",
        "\n",
        "############## Fireing Rate Gain ################\n",
        "print('Fireing Rate Gain...')\n",
        "IFcurve_aliases = ['IC - Gain - D20pA', 'IC - Gain - D50pA']\n",
        "abf_recordings_df = IF_curve_analysis(abf_recordings_df, IFcurve_aliases,to_plot = 0)\n",
        "\n",
        "############## SpikeLatency ################\n",
        "print('SpikeLatency...')\n",
        "SpikeLatency_aliases = ['IC - Latentcy 800pA-1s']\n",
        "abf_recordings_df = Spike_latency(abf_recordings_df, SpikeLatency_aliases,to_plot = False)\n",
        "\n",
        "################### IClamp Input Resistance ##############\n",
        "print('IClamp Input Resistance...')\n",
        "inputR_aliases = ['IC - R input']\n",
        "abf_recordings_df = inputR_analysis(abf_recordings_df, inputR_aliases,to_plot=False)\n",
        "\n",
        "################### IV Curve ###################\n",
        "print('IV Curve...')\n",
        "protocol_aliases = ['VC - Multi IV - 150ms']\n",
        "abf_recordings_df = IV_analyisis(abf_recordings_df, protocol_aliases,to_plot=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QlTGCr9eKCcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################\n",
        "#################### Consolidate and Summarize Data ##################\n",
        "########################################################################\n",
        "cell_summary_df_clean, abrg_abf_recs_df = consolidate_store_data(abf_recordings_df)\n",
        "# display(abrg_abf_recs_df)"
      ],
      "metadata": {
        "id": "zN9Fzc9Qmagu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "19f3f45e-fd04-4b55-d539-d3a381bf8e3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30  cells from 4 animals\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_03f8db9c-d869-476a-85c4-77869a06f344\", \"abrg_abf_recs_df.csv\", 84099)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c5fafd9e-959e-403d-a696-fb7deab14694\", \"abf_recordings_df_tagged.csv\", 84099)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_2918caf6-e569-4c25-8906-d730263040c4\", \"cell_summary_df.csv\", 63983)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_35be1df0-845f-44cb-b52f-543fd844e07d\", \"cell_summary_df_clean.csv\", 33401)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_col(df_old,col_name,verbose=False,default_names=True):\n",
        "    'Blow up composite columns into individual columns'\n",
        "    df = df_old.copy()\n",
        "    ser_to_exp = df[col_name]\n",
        "    def safe_len(x):\n",
        "        try: return len(x)\n",
        "        except: return 0\n",
        "    \n",
        "    new_col_num = np.max( [safe_len(i) for i in ser_to_exp])\n",
        "    new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    if default_names:\n",
        "        new_names = [col_name +'_'+str(i) for i in range(new_col_num)]\n",
        "    else:\n",
        "        new_names = default_names\n",
        "    \n",
        "    for ni in range(new_col_num):\n",
        "        name = new_names[ni]\n",
        "        df[name] = np.nan\n",
        "    for row in df.index:\n",
        "        for i in range(safe_len(df.loc[row,col_name])):\n",
        "            df.at[row,new_names[i]] = df.loc[row,col_name][i]\n",
        "\n",
        "    \n",
        "    old_cols = list(df_old.columns)\n",
        "    old_col_ind = [i for i in range(len(old_cols)) if col_name in old_cols[i]][0]\n",
        "\n",
        "    \n",
        "    new_order = old_cols[:old_col_ind] + new_names + old_cols[old_col_ind+1:]\n",
        "    df = df[new_order]\n",
        "    if verbose: _ = [print(c) for c in cell_summary_exl_friendly.columns]\n",
        "    return df\n",
        "\n",
        "\n",
        "cell_summary_exl_friendly = cell_summary_df_clean.copy()\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_Stims_pA')\n",
        "cell_summary_exl_friendly = expand_col(cell_summary_exl_friendly,'Gain_NumSpikes')\n",
        "cell_summary_exl_friendly.to_csv('cell_summary_exl_friendly.csv')\n",
        "files.download('cell_summary_exl_friendly.csv')"
      ],
      "metadata": {
        "id": "BbtXnXhPvD8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cell_summary_exl_friendly.columns)"
      ],
      "metadata": {
        "id": "wf5PGzqBmhhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### PCA CLUSTER ##########\n",
        "\n",
        "# _ = [print(c,cell_summary_exl_friendly.columns[c]) for c in range(len(cell_summary_exl_friendly.columns))]\n",
        "col_inds_for_pca = [9] + [12] + list(range(13,17)) + [18] + list(range(31,40))\n",
        "cols_for_pca = cell_summary_exl_friendly.columns[col_inds_for_pca]\n",
        "\n",
        "cols_for_pca = [ 'Ra_160.0', 'Rm_160.0','Cmq_10.0', 'Cmq_160.0','Spike_Latency_(ms)', 'Rheobase_(pA)',\n",
        "                'AP_Threshold(mV)', 'Rinput_(MO)', 'Firing_Gain_(Hz/pA)',\n",
        "                'Firing_Duration_%' ] #,'Gain_NumSpikes_0', 'Gain_NumSpikes_1',\n",
        "                # 'Gain_NumSpikes_2', 'Gain_NumSpikes_3', 'Gain_NumSpikes_4',\n",
        "                # 'Gain_NumSpikes_5', 'Gain_NumSpikes_6', 'Gain_NumSpikes_7',\n",
        "                # 'Gain_NumSpikes_8', 'Gain_NumSpikes_9']\n",
        "\n",
        "\n",
        "\n",
        "# _ = [print(c) for c in cols_for_pca]\n",
        "df = cell_summary_exl_friendly[cols_for_pca]\n",
        "# display(df)\n",
        "X_ = df.to_numpy(dtype='float32')\n",
        "nan_bool = np.logical_not(np.isnan(X_))\n",
        "row_nan = np.all(nan_bool,axis=1)\n",
        "df = df[row_nan]\n",
        "# display(df)\n",
        "X_ = X_[row_nan,:]\n",
        "# print(X_)\n",
        "\n",
        "\n",
        "import sklearn.decomposition\n",
        "import sklearn.cluster\n",
        "\n",
        "pca = sklearn.decomposition.PCA(n_components=np.min(X_.shape)).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "pca = sklearn.decomposition.PCA(n_components=3).fit(X_)\n",
        "print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "X_trans = pca.transform(X_)\n",
        "X_red = pca.inverse_transform(X_trans)\n",
        "\n",
        "\n",
        "RANDOM_SEED = 5\n",
        "kmeans = sklearn.cluster.KMeans(n_clusters=2,random_state=RANDOM_SEED).fit(X_trans)\n",
        "Centroids_red = pca.inverse_transform(kmeans.cluster_centers_)\n",
        "null_kmeans = sklearn.cluster.KMeans(n_clusters=1).fit(X_trans)\n",
        "null_centoid =  pca.inverse_transform(null_kmeans.cluster_centers_)[0]\n",
        "# print(Centroids_red)\n",
        "# print(null_centoid)\n",
        "\n",
        "type_0_cent = 100*(Centroids_red[0] / null_centoid - 1)\n",
        "type_1_cent = 100*(Centroids_red[1] / null_centoid - 1)\n",
        "\n",
        "X_rel = (100*(X_ / null_centoid - 1))\n",
        "\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(12, 6))\n",
        "x = np.arange(len(cols_for_pca))*-1\n",
        "\n",
        "wid = .3\n",
        "\n",
        "\n",
        "is_type_0 = kmeans.labels_ == 0\n",
        "is_type_1 = kmeans.labels_ == 1\n",
        "X_at0 = X_rel[is_type_0,:]  \n",
        "X_at1 = X_rel[is_type_1,:] \n",
        "\n",
        "\n",
        "axs[0].barh(x-wid/2, type_0_cent, wid,color = 'turquoise',label='Ephys type_0'+' n=' +str(sum(is_type_0)))\n",
        "axs[0].barh(x+wid/2, type_1_cent, wid,color = 'magenta',label='Ephys type_1'+' n=' +str(sum(is_type_1)))\n",
        "\n",
        "x_exp0 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_0), axis=0)\n",
        "axs[1].plot(X_at0.transpose(), x_exp0.transpose(), '-.',color = 'turquoise')\n",
        "\n",
        "x_exp1 = np.repeat(np.expand_dims(x, axis=0),repeats =sum(is_type_1), axis=0)\n",
        "axs[1].plot(X_at1.transpose(), x_exp1.transpose(), '-.',color = 'magenta' )\n",
        "\n",
        "for ax in axs:\n",
        "    axs[0].legend()\n",
        "    ax.set_yticks(x)\n",
        "    axs[0].set_yticklabels(cols_for_pca, rotation=0)\n",
        "    ax.set_xlabel('% Diff Global Mean')\n",
        "plt.show()\n",
        "\n",
        "fig.savefig('clusters.png',dpi=300)\n",
        "files.download('clusters.png')"
      ],
      "metadata": {
        "id": "Bvw7XZ7fz13F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(X_trans)\n",
        "# print(X_trans[is_type_0])\n",
        "\n",
        "fig, ax = plt.subplots(3,3,figsize = [4,4])\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        ax[i,j].scatter(X_trans[is_type_0][:,i],X_trans[is_type_0][:,j],color='turquoise')\n",
        "        ax[i,j].scatter(X_trans[is_type_1][:,i],X_trans[is_type_1][:,j],color='magenta')\n"
      ],
      "metadata": {
        "id": "OlkBcnLBv3-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}