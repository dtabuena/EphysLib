{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUdUn+XmNf+OsWYXqUhjfU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtabuena/EphysLib/blob/main/Pipeline_Wrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P13PuWPaRl5I"
      },
      "outputs": [],
      "source": [
        "def ephys_wrapper(dataset,VC_prot,IC_prot,strat_cols=['Cell_Type'],verbose=False, spike_args=True):\n",
        "    '''wrapper for single dataset pipeline'''\n",
        "    results = {}\n",
        "    # try:\n",
        "    '''Unpack'''\n",
        "    data_name = dataset['data_name']\n",
        "    data_source = dataset['data_source']\n",
        "    file_naming_scheme = dataset['file_naming_scheme']\n",
        "\n",
        "\n",
        "\n",
        "    ''' Gather and Catalog Source Data'''\n",
        "    file_loc = get_drobox_folder(data_source, 'my_ephys_data_' + data_name)\n",
        "    # clear_output(wait=False)   \n",
        "    abf_recordings_df, protocol_set = catalogue_recs(file_loc,file_naming_scheme)\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "    results['protocol_set'] = protocol_set\n",
        "\n",
        "    abf_recordings_df, _ = purge_wrong_clamp(abf_recordings_df,VC_prot,IC_prot)\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "    _ = cell_prot_lut(abf_recordings_df,protocol_set,csv_name=data_name+'_Recording_LookUp')\n",
        "\n",
        "\n",
        "    '''Set Internal Analysis Params'''\n",
        "    if spike_args:\n",
        "        spike_args =  {'spike_thresh':20, 'high_dv_thresh': 50,'low_dv_thresh': -30,'window_ms': 2}\n",
        "\n",
        "    func_dict = {}\n",
        "    arg_dict = {}\n",
        "\n",
        "    func_dict['VC - 3min GapFree']= rmp_analyzer\n",
        "    arg_dict['VC - 3min GapFree'] = [True] # [to_plot?]\n",
        "\n",
        "    func_dict['IC - Rheobase']= rheobase_analyzer\n",
        "    arg_dict['IC - Rheobase'] = [spike_args, True, False, False]  # [spike_args, to_plot, verbose, force_singlespike]\n",
        "\n",
        "    func_dict['IC - Gain - D20pA']= gain_analyzer\n",
        "    arg_dict['IC - Gain - D20pA']= [spike_args, 1]  # [spike_args, to_plot [0:2],]\n",
        "    func_dict['IC - Gain - D50pA']= func_dict['IC - Gain - D20pA'] \n",
        "    arg_dict['IC - Gain - D50pA']= arg_dict['IC - Gain - D20pA']\n",
        "\n",
        "    func_dict['VC - MemTest-10ms-160ms']= membrane_analyzer\n",
        "    arg_dict['VC - MemTest-10ms-160ms']= [True, False, ['Ra', 'Rm', 'Cm', 'tau',\t'Cmq',\t'Cmf',\t'Cmqf', 'Cm_pc']]  # [to_plot, verbose]\n",
        "\n",
        "    func_dict['IC - Latentcy 800pA-1s']= latencey_analyzer \n",
        "    arg_dict['IC - Latentcy 800pA-1s']= [spike_args, True]  # [spike_args, to_plot]\n",
        "\n",
        "    func_dict['IC - R input']= input_resistance_analyzer \n",
        "    arg_dict['IC - R input']= [[-30, 10] ,True]  # [dVm_limits, to_plot]\n",
        "\n",
        "    func_dict['VC - Multi IV - 150ms'] = IV_analyzer_v2\n",
        "    arg_dict['VC - Multi IV - 150ms']= [{'IV_Early':(16.5, 30),'IV_Steady_State':(100,120)} ,[False, True]]  # [measure_windows, to_plot]\n",
        "\n",
        "\n",
        "    '''Analyze Dataset'''\n",
        "    abf_recordings_df, problem_recs = analysis_iterator(abf_recordings_df,func_dict,arg_dict,verbose=verbose)\n",
        "    # clear_output(wait=True)\n",
        "    print('problem_recs')\n",
        "    _=[print('     '+r) for r in problem_recs]\n",
        "    results['problem_recs'] = problem_recs\n",
        "    results['abf_recordings_df'] = abf_recordings_df\n",
        "\n",
        "    '''Download Analysis figs'''\n",
        "    zip_name = '/content/' + data_name + '_Saved_Figs.zip'\n",
        "    !zip -r $zip_name /content/Saved_Figs \n",
        "    # clear_output()\n",
        "    files.download(data_name + '_Saved_Figs.zip')\n",
        "\n",
        "    '''Sort Cells'''\n",
        "    cell_df = cell_sorting(abf_recordings_df)\n",
        "    results['cell_df'] = cell_df\n",
        "\n",
        "    '''Consolidate to Cells'''\n",
        "    list_types = ['Recording_name','protocol','abf_timestamp', 'channelList']\n",
        "    any_types = [] + dataset['file_naming_scheme']\n",
        "    cell_df_con = cell_consolidation(cell_df,list_types,any_types)\n",
        "    results['cell_df_con'] = cell_df_con\n",
        "\n",
        "    '''Simplify IV Data'''\n",
        "    cols_to_simplify = ['IV_Early', 'IV_Steady_State']\n",
        "    cell_df_nd = simplify_dicts(cell_df_con,cols_to_simplify)     \n",
        "    results['cell_df_nd'] = cell_df_nd\n",
        "\n",
        "    '''Make Excell Friendly'''\n",
        "    keys_and_data_cols={'Stim_Levels_(pA)': ['Stim_Levels_(pA)', 'Spike_Counts' ],\n",
        "                    'IV_Early_(V_stim)': ['IV_Early_(V_stim)', 'IV_Early_(I_peak)', 'IV_Steady_State_(I_mean)']}\n",
        "    cell_df_csv = csv_frinedly(cell_df_nd,keys_and_data_cols)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "\n",
        "    ''' Convert to Current Density'''\n",
        "    size_col = 'Cmq_160.0'\n",
        "    current_col_list = ['IV_Early_(I_peak)_', 'IV_Steady_State_(I_mean)_']\n",
        "    cell_df_csv = current_density_correction(cell_df_csv, size_col, current_col_list)\n",
        "    results['cell_df_csv'] = cell_df_csv\n",
        "\n",
        "    '''Abridge DataFrame'''\n",
        "    abrg_exclusions = ['Recording_name', \n",
        "                    'protocol', 'abf_timestamp', 'channelList',  'Ra_10.0', 'Rm_10.0', 'tau_10.0', 'Cmq_10.0', 'Cmf_10.0',\n",
        "                    'Cmqf_10.0',  'Cmf_160.0', 'Cmqf_160.0', 'Cm_pc_160.0',\n",
        "                    'Gain_R2', 'Stim_Levels_(pA)', 'Spike_Counts', 'Firing_Duration_%', 'Gain_Vh',  'Vhold_spike',\n",
        "                        'Rin_Rsqr',  'Ramp_AP_thresh', 'Ramp_Vh', 'Ramp_Rheobase', \n",
        "                    'ap_thresh_us', 'v_half','is_compensated','sum_delta'\n",
        "                    'IV_Early_(range)', 'IV_Early_(I_peak)', 'IV_Early_(I_mean)', 'IV_Early_(V_stim)', 'IV_Steady_State_(range)',\n",
        "                    'IV_Steady_State_(I_peak)', 'IV_Steady_State_(I_mean)', 'IV_Steady_State_(V_stim)', ]\n",
        "\n",
        "    abrg_keep = [c for c in cell_df_csv.columns if c not in abrg_exclusions]\n",
        "    cell_df_csv_abrg = cell_df_csv[abrg_keep]\n",
        "    results['cell_df_csv_abrg'] = cell_df_csv_abrg\n",
        "\n",
        "    '''Stratify Cells By Type'''\n",
        "    strat_df_dict = stratify_rec(cell_df_csv_abrg,strat_cols)\n",
        "    strat_df_dict,_ = flatten_dict(strat_df_dict,{})\n",
        "    write_strat_dfs(strat_df_dict, dataset['data_name']+'_results_stratified')\n",
        "    results['strat_df_dict'] = strat_df_dict\n",
        "    # except: None\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def csv_frinedly(cell_df,keys_and_data_cols,remove_source = True):\n",
        "    cell_df_csv = cell_df.copy()\n",
        "    for k in keys_and_data_cols.keys():\n",
        "        for data_col in keys_and_data_cols[k]:\n",
        "            for cell in cell_df_csv.index:\n",
        "                label_value_list = cell_df_csv.loc[cell,k]\n",
        "                data_value_list = cell_df_csv.loc[cell,data_col]\n",
        "                if label_value_list is None: continue\n",
        "                label_value_len = len( label_value_list)\n",
        "                for i in range(label_value_len):\n",
        "                    new_col_name = data_col + '_' + str( cell_df_csv.loc[cell,k][i])\n",
        "                    if new_col_name not in cell_df_csv.columns: cell_df_csv[new_col_name] = None\n",
        "                    cell_df_csv.at[cell,new_col_name] = data_value_list[i]\n",
        "\n",
        "    return cell_df_csv"
      ],
      "metadata": {
        "id": "iX1u6mUXR0P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analysis_iterator(abf_recordings_df,func_dict,arg_dict,verbose=False):\n",
        "    problem_recs = []\n",
        "    \n",
        "\n",
        "    for file_name in tqdm(abf_recordings_df.index):\n",
        "        \n",
        "        abf = abf_or_name(file_name)\n",
        "        prot_name = abf.protocol\n",
        "        if verbose: print('\\n','     ',file_name)\n",
        "        if verbose: print('     ',prot_name)\n",
        "\n",
        "        # check for keyed protocol\n",
        "        if prot_name not in func_dict.keys():\n",
        "            # print('unknown protocol(func): ',  prot_name)\n",
        "            continue\n",
        "        if prot_name not in arg_dict.keys():\n",
        "            # print('unknown protocol(args): ',  prot_name)\n",
        "            continue\n",
        "\n",
        "\n",
        "        try:\n",
        "            results = analyzer_func(*args_for_analyzer) # run analyzer\n",
        "        except:\n",
        "            print('\\n','error on: ' ,file_name)\n",
        "            print('analysis failed')\n",
        "        try:\n",
        "            for k in results.keys():\n",
        "                # New Col?\n",
        "                cols = abf_recordings_df.columns\n",
        "                if k not in cols:\n",
        "                    abf_recordings_df = init_col_object(abf_recordings_df,k)\n",
        "                abf_recordings_df.at[file_name,k] = results[k]\n",
        "        except: \n",
        "            print('\\n','error on: ' ,file_name)\n",
        "            print('recording results failed')\n",
        "            problem_recs.append(file_name)\n",
        "\n",
        "    return abf_recordings_df, problem_recs\n",
        "\n",
        "def init_col_object(df,name): \n",
        "        df[name] = None\n",
        "        df[name] = df[name].astype(object)\n",
        "        return df"
      ],
      "metadata": {
        "id": "bXrnQJxCR2In"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_sorting(abf_recordings_df):\n",
        "\n",
        "    unique_cells = list(set(abf_recordings_df['cell_id']))\n",
        "    unique_cells.sort()\n",
        "    transfer_cols = [c for c in abf_recordings_df.columns if 'cell_id' not in c]\n",
        "    cell_df = pd.DataFrame(index=list(unique_cells),columns = transfer_cols)\n",
        "    \n",
        "\n",
        "    for cell in cell_df.index:\n",
        "        match = [cell in r for r in abf_recordings_df['cell_id']]\n",
        "        for col in transfer_cols:\n",
        "            match_values = list(abf_recordings_df[match][col])\n",
        "            # print('col', col)\n",
        "            # print(match_values)\n",
        "\n",
        "            cell_df.at[cell,col] = match_values\n",
        "    return cell_df"
      ],
      "metadata": {
        "id": "RVxn4xQzR46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cell_consolidation(cell_df,list_types,any_types,average_types = True):\n",
        "    cell_df_con = cell_df.copy()\n",
        "    explicit_cols = ['IV_Early','IV_Steady_State','Stim_Levels_(pA)','Spike_Counts']\n",
        "\n",
        "    if average_types:\n",
        "        average_types = [c for c in cell_df_con.columns if c not in any_types and c not in list_types and c not in explicit_cols]\n",
        "        \n",
        "        # print('average_types',average_types)\n",
        "\n",
        "\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        for col in list_types:\n",
        "            'do nothing, keep the list'\n",
        "        for col in any_types:\n",
        "            'they are all the same take the first'\n",
        "            cell_df_con.at[cell,col] = cell_df_con.at[cell,col][0]\n",
        "\n",
        "        for col in average_types:\n",
        "            multi_vals = cell_df_con.loc[cell,col]\n",
        "            try:\n",
        "                multi_vals = [v for v in multi_vals if v is not None]\n",
        "                single_val = np.nanmean(multi_vals,0)\n",
        "                cell_df_con.at[cell,col] = single_val\n",
        "                # print(single_val)\n",
        "            except: 'Just keep going None'\n",
        "        \n",
        "\n",
        "    # explicitly defined consolidations\n",
        "    for col in ['IV_Early', 'IV_Steady_State']:\n",
        "        # assert col in cell_df_con.index, f\"Column to consoidate not found: {col}\"\n",
        "        for cell in cell_df_con.index:\n",
        "            try:\n",
        "                multi_vals = cell_df_con.loc[cell,col]\n",
        "                multi_vals = consolidate_iv_recs(multi_vals)\n",
        "            except:\n",
        "                if np.isnan(multi_vals): multi_vals = None\n",
        "                else: multi_vals = 'ERROR'       \n",
        "\n",
        "            if not isinstance(multi_vals, list): multi_vals=[multi_vals]\n",
        "            cell_df_con.at[cell,col] = multi_vals\n",
        "\n",
        "    for cell in cell_df_con.index:\n",
        "        multi_val_pair = (cell_df_con.loc[cell,'Stim_Levels_(pA)'], cell_df_con.loc[cell,'Spike_Counts'])\n",
        "        multi_val_pair = consolidate_gain_recs(multi_val_pair)\n",
        "\n",
        "        new_stim = multi_val_pair[0]\n",
        "        new_firing = multi_val_pair[1]\n",
        "        if len(new_stim)>0:\n",
        "            if isinstance(new_stim[0],list):\n",
        "                new_stim = new_stim[0]\n",
        "        if len(new_firing)>0:\n",
        "            if isinstance(new_firing[0],list):\n",
        "                new_firing = new_firing[0]\n",
        "\n",
        "        cell_df_con.at[cell,'Stim_Levels_(pA)'] = new_stim\n",
        "        cell_df_con.at[cell,'Spike_Counts'] = new_firing\n",
        "\n",
        "    # cell_df_con = cell_df_con.reindex(sorted(cell_df_con.columns), axis=1)\n",
        "    return cell_df_con"
      ],
      "metadata": {
        "id": "fVZb4YzeR7Ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_iv_recs(multi_vals):\n",
        "    multi_vals = [v for v in multi_vals if v is not None]   \n",
        "    v_stim = [  mv['V_stim'] for mv in  multi_vals ]\n",
        "    peak_vals = [  mv['I_peak'] for mv in  multi_vals ]\n",
        "    if len(v_stim)>1:\n",
        "        rec_lengths = [len(v) for v in v_stim]\n",
        "        long_enough = np.where(np.array(rec_lengths) > 5)[0][0]\n",
        "        multi_vals = multi_vals[long_enough]\n",
        "        # print(multi_vals)  \n",
        "    return multi_vals"
      ],
      "metadata": {
        "id": "2LoRN07nR9_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def simplify_dicts(cell_df,cols_to_simplify,remove_source = True):\n",
        "    cell_df_new = cell_df.copy()\n",
        "    for col in cols_to_simplify:\n",
        "        for cell in cell_df_new.index:\n",
        "            list_of_dicts = cell_df_new.loc[cell,col]\n",
        "            list_of_dicts = [d for d in list_of_dicts if d is not None]\n",
        "            if len(list_of_dicts) == 0: continue\n",
        "            # print(list_of_dicts)\n",
        "            list_of_keys = list(list_of_dicts[0].keys())            \n",
        "            for k in list_of_keys:\n",
        "                vals_of_key = []\n",
        "                for i in range(len(list_of_dicts)):\n",
        "                    vals_of_key.append(  list_of_dicts[i][k] )\n",
        "                if len(vals_of_key) == 1: vals_of_key = vals_of_key[0]\n",
        "                new_col = col + '_(' + str(k) +')'\n",
        "                if new_col not in cell_df_new.columns: \n",
        "                    cell_df_new[new_col] = None\n",
        "                    cell_df_new[new_col] = cell_df_new[new_col].astype(object)\n",
        "                cell_df_new.at[cell,new_col] = vals_of_key\n",
        "        cell_df_new.drop(labels=col, axis = 1,inplace = True)\n",
        "    return cell_df_new"
      ],
      "metadata": {
        "id": "dC_Ts2jvR_zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def consolidate_gain_recs(multi_val_pair):\n",
        "    min_stims = 5\n",
        "    mv_stim = multi_val_pair[0]\n",
        "    mv_fire = multi_val_pair[1]\n",
        "    mv_stim = [v.tolist() for v in mv_stim if v is not None]\n",
        "    mv_fire = [v.tolist() for v in mv_fire if v is not None]\n",
        "    results = (mv_stim, mv_fire)\n",
        "\n",
        "\n",
        "    if len(mv_stim)>1:\n",
        "        rec_lengths = [len(v) for v in mv_stim]\n",
        "        mv_stim = [v for v in mv_stim if len(v) >=min_stims]\n",
        "        mv_fire = [v for v in mv_fire if len(v) >=min_stims]\n",
        "\n",
        "    results = (mv_stim, mv_fire)\n",
        "    \n",
        "    if len(mv_stim)>1:\n",
        "        stim_set = list(set( [vv for v in mv_stim for vv in v] ))# flat_list = [item for sublist in regular_list for item in sublist]\n",
        "        stim_set.sort()\n",
        "        new_vals_dict = {}\n",
        "        for s in stim_set:\n",
        "            matching_response =[]\n",
        "            matching_stim = []\n",
        "            for i in range(len(mv_stim)):\n",
        "                for j in range(len(mv_stim[i])):\n",
        "                    if mv_stim[i][j] == s:\n",
        "                        matching_stim.append(mv_stim[i][j])\n",
        "                        matching_response.append(mv_fire[i][j])\n",
        "            new_vals_dict[s] =  matching_response\n",
        "        new_stim_list = []\n",
        "        new_response_list = []\n",
        "        for k in new_vals_dict:\n",
        "            new_vals_dict[k] = np.mean(new_vals_dict[k])\n",
        "            new_stim_list.append(k)\n",
        "            new_response_list.append(new_vals_dict[k])\n",
        "\n",
        "\n",
        "        results = (new_stim_list, new_response_list)\n",
        "        \n",
        "    return results"
      ],
      "metadata": {
        "id": "o-SIlwfESCYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def current_density_correction(cell_df,size_col,current_col_list,remove_old=True):\n",
        "    cell_df_cd = cell_df.copy()\n",
        "    ccl_exp = []\n",
        "    for ccl in current_col_list:\n",
        "        ccl_exp = ccl_exp + [c for c in cell_df.columns if ccl in c]\n",
        "    current_col_list = ccl_exp\n",
        "    for cell in cell_df.index:\n",
        "        size = cell_df.loc[cell,size_col]\n",
        "        for col in current_col_list:\n",
        "            try:\n",
        "                new_col = col +'_pApF'\n",
        "                cell_df_cd.at[ cell,new_col] = cell_df_cd.at[ cell,col] / size\n",
        "            except: \n",
        "                cell_df_cd.at[ cell,new_col] = None\n",
        "    \n",
        "    cell_df_cd = cell_df_cd[[ c for c in cell_df_cd.columns if c not in current_col_list ]]\n",
        "\n",
        "    return cell_df_cd"
      ],
      "metadata": {
        "id": "OMOnzIYhSFiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stratify_cells(cell_df,strat_col,xl_file_name='stratified_data.xlsx'):\n",
        "    types = list(set(cell_df[strat_col]))\n",
        "\n",
        "    new_dfs = {}\n",
        "    options = {}\n",
        "    options['strings_to_formulas'] = False\n",
        "    options['strings_to_urls'] = False\n",
        "    writer = pd.ExcelWriter(xl_file_name, options=options)\n",
        "    for t in types:\n",
        "        is_type = cell_df[strat_col] == t\n",
        "        new_dfs[t] = cell_df[is_type]\n",
        "        new_dfs[t].to_excel(writer, sheet_name=str(t))\n",
        "        # new_dfs[t].to_csv(str(t) + '_cell_df_csv.csv')\n",
        "        # files.download(str(t) + '_cell_df_csv.csv')\n",
        "    writer.save()\n",
        "    writer.close()\n",
        "    files.download(xl_file_name)\n",
        "    return new_dfs"
      ],
      "metadata": {
        "id": "x5bFBjOISHEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def stratify_rec(cell_df,strat_col,prefix=''):\n",
        "    strat_dfs = {}\n",
        "\n",
        "\n",
        "    cur_col = strat_col[0]\n",
        "    rem_col = [c for c in strat_col if c not in cur_col]\n",
        "\n",
        "    if len(cur_col) == 0:\n",
        "        return df\n",
        "\n",
        "    types = list(set(cell_df[cur_col]))\n",
        "    for t in types:\n",
        "        is_type = cell_df[cur_col] == t\n",
        "        df_name = prefix +'_'+t\n",
        "        strat_dfs[df_name] = cell_df[is_type]\n",
        "        new_to_strat = strat_dfs[df_name].copy()\n",
        "        for r in rem_col:            \n",
        "            strat_dfs[df_name] = stratify_rec(new_to_strat,rem_col,prefix=t)\n",
        "\n",
        "    return strat_dfs"
      ],
      "metadata": {
        "id": "mkvUMw_rSIrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flatten_dict(my_dict,flat_dict = {} ):\n",
        "    for k in my_dict.keys():\n",
        "        if isinstance(my_dict[k], dict):\n",
        "            sub_dict, sub_keys = flatten_dict(my_dict[k],flat_dict)\n",
        "            for sk in sub_keys:\n",
        "                flat_dict[k+'_'+sk] = sub_dict[sk]\n",
        "        else:\n",
        "            flat_dict = my_dict\n",
        "    return flat_dict, list(flat_dict.keys())"
      ],
      "metadata": {
        "id": "oV4udxgUSKe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_strat_dfs(strat_dfs, xl_file_name='stratified_data.xlsx'):\n",
        "    if '.xlsx' not in xl_file_name: xl_file_name = xl_file_name+'.xlsx'\n",
        "    options = {}\n",
        "    options['strings_to_formulas'] = False\n",
        "    options['strings_to_urls'] = False\n",
        "    writer = pd.ExcelWriter(xl_file_name, options=options)\n",
        "    for k in strat_dfs.keys():\n",
        "        cur_df = strat_dfs[k]\n",
        "        # cur_df = strat_dfs[k].T\n",
        "        cur_df.to_excel(writer, sheet_name=str(k))\n",
        "    writer.save()\n",
        "    writer.close()\n",
        "    files.download(xl_file_name)\n",
        "    return None"
      ],
      "metadata": {
        "id": "5tV2iw9FSMaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}